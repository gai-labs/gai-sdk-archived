{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Code (TTC)\n",
    "\n",
    "## 1.1 Setting Up\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "conda create -n TTC python=3.10.10 -y\n",
    "conda activate TTC\n",
    "pip install -e \".[TTC]\"\n",
    "```\n",
    "\n",
    "NOTE: The installation depends on requirements_ttc.txt which is based on https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/requirements.txt\n",
    "\n",
    "Download the deepseek coder model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download TheBloke/deepseek-coder-6.7B-instruct-GPTQ \\\n",
    "        --local-dir ~/gai/models/deepseek-coder-6.7b-instruct \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install autogptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a new virtual environment TTC\n",
    "\n",
    "Based on TTT, except that we will be using auto-gptq instead of exllama\n",
    "\n",
    "Refer to requirements_ttc.txt\n",
    "\n",
    "### config\n",
    "\n",
    "```json\n",
    "        \"deepseek-gptq\": {\n",
    "            \"type\": \"ttc\",\n",
    "            \"model_name\": \"deepseek-coder-6.7B\",\n",
    "            \"engine\": \"Deepseek_TTC\",\n",
    "            \"model_path\": \"models/deepseek-coder-6.7B-instruct-GPTQ\",\n",
    "        ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ttc dir and TTC generator\n",
    "\n",
    "1. Create a new dir called ttc\n",
    "2. Create ExLlama2_TTC.py based on ExLlama_TTT.py (only difference is change the import of ExLlama to ExLlama2)\n",
    "3. Create TTC.py generator based on TTT.py. Update the reference to point to the new config section \"deepseek-exllama\" and ExLlama2 generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Running as a Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roylai/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/roylai/miniconda/envs/TTC/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/roylai/miniconda/envs/TTC/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-04-21 15:39:57 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator deepseek-gptq...\u001b[0m\n",
      "2024-04-21 15:39:57 INFO gai.gen.ttc.TTC:\u001b[32mUsing ttc model Deepseek_TTC...\u001b[0m\n",
      "2024-04-21 15:39:57 INFO gai.gen.ttc.Deepseek_TTC:\u001b[32mLoading model from /home/roylai/gai/deepseek-coder-6.7B-instruct-GPTQ/model.safetensors\u001b[0m\n",
      "\u001b[33mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "INFO - \u001b[32mThe layer lm_head is not quantized.\u001b[0m\n",
      "2024-04-21 15:39:58 INFO auto_gptq.modeling._base:\u001b[32m\u001b[32mThe layer lm_head is not quantized.\u001b[0m\u001b[0m\n",
      "2024-04-21 15:40:12 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC.create: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-04-21 15:40:12 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: prompt=USER: Create a quick sort function in python.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-04-21 15:40:12 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: {'max_new_tokens': 100, 'do_sample': True, 'early_stopping': False, 'encoder_repetition_penalty': 1, 'eos_token_id': 32021, 'length_penalty': 1, 'logits_processor': [], 'min_length': 0, 'no_repeat_ngram_size': 0, 'num_beams': 1, 'penalty_alpha': 0, 'repetition_penalty': 1.17, 'temperature': 1.2, 'top_k': 50, 'top_p': 0.15, 'typical_p': 1}\u001b[0m\n",
      "2024-04-21 15:40:12 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: input token count=15\u001b[0m\n",
      "2024-04-21 15:40:21 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: raw output=USER: Create a quick sort function in python.\n",
      "ASSISTANT: Sure, here is an example of how you can implement the quicksort algorithm using Python:\n",
      "```python\n",
      "def partition(arr, low, high): \n",
      "    i = (low-1)         # index of smaller element \n",
      "    pivot = arr[high]     # pivot \n",
      "  \n",
      "    for j in range(low , high): \n",
      "        if   arr[j] <= pivot: \n",
      "            i += 1 \n",
      "            arr[\u001b[0m\n",
      "2024-04-21 15:40:21 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: output token count=115\u001b[0m\n",
      "2024-04-21 15:40:21 INFO gai.gen.ttc.Deepseek_TTC:\u001b[32mDeepseek_TTC: output token count=115  < max_new_tokens: 100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load Instance\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('deepseek-gptq')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Create a quick sort function in python.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 15:42:09 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: Generator is already loaded. Skip loading.\u001b[0m\n",
      "2024-04-21 15:42:09 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC.create: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 4000, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-04-21 15:42:09 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: prompt=USER: Create a python websocket server using fastAPI and python websocket client that connects to it.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-04-21 15:42:09 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: {'max_new_tokens': 4000, 'do_sample': True, 'early_stopping': False, 'encoder_repetition_penalty': 1, 'eos_token_id': 32021, 'length_penalty': 1, 'logits_processor': [], 'min_length': 0, 'no_repeat_ngram_size': 0, 'num_beams': 1, 'penalty_alpha': 0, 'repetition_penalty': 1.17, 'temperature': 1.2, 'top_k': 50, 'top_p': 0.15, 'typical_p': 1}\u001b[0m\n",
      "2024-04-21 15:42:09 DEBUG gai.gen.ttc.Deepseek_TTC:\u001b[35mDeepseek_TTC: input token count=26\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load Instance\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('deepseek-gptq')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Create a python websocket server using fastAPI and connect to it using python websockets.'},\n",
    "                                {'role':'ASSISTANT','content':''}]\n",
    "                                ,max_new_tokens=4000,\n",
    "                                stream=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
