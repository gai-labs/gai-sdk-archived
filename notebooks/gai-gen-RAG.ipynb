{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Retrieval-Augmented-Generation (RAG)\n",
    "\n",
    "## 1. Note\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "\n",
    "-   NVidia GeForce RTX 2060 6GB\n",
    "-   Windows 11 + WSL2\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "\n",
    "## 2. Create Virtual Environment and Install Dependencies\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n RAG python=3.10.10 -y\n",
    "conda activate RAG\n",
    "pip install -e \".[RAG]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download hkunlp/instructor-large \\\n",
    "        --local-dir ~/gai/models/instructor-large \\\n",
    "        --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index and Retrieve Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roylai/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-22 20:55:13 INFO gai.gen.rag.RAG:\u001b[32mDeleting demo...\u001b[0m\n",
      "2024-02-22 20:55:13 WARNING gai.gen.rag.RAG:\u001b[33mdelete_collection: Collection demo does not exist.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Reset 'demo' collection\n",
    "from gai.gen.rag import RAG\n",
    "rag = RAG()\n",
    "rag.delete_collection(\"demo\")\n",
    "rag.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 681.09it/s]\n",
      "2024-02-22 20:55:17 INFO gai.gen.rag.RAG:\u001b[32mRAG.index_async: Begin indexing...\u001b[0m\n",
      "0it [00:00, ?it/s]2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 1/29 chunk 5d98b767-67d4-4058-b6b2-cd18b452d741 into collection demo\u001b[0m\n",
      "1it [00:01,  1.64s/it]2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 2/29 chunk 6980715e-c444-4533-94f1-d05f6d91bb2a into collection demo\u001b[0m\n",
      "2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 3/29 chunk 8d7af7f5-e57a-4b37-b2be-135ae5afbb1e into collection demo\u001b[0m\n",
      "3it [00:01,  2.07it/s]2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 4/29 chunk 0580bbe1-1448-47e4-876a-d91c2fcdf5e8 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 5/29 chunk be0811e0-64eb-4d3a-95d1-28a880b657bc into collection demo\u001b[0m\n",
      "5it [00:01,  3.61it/s]2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 6/29 chunk f71e7c22-be36-4b8c-b340-8ac3f18cf2c0 into collection demo\u001b[0m\n",
      "6it [00:02,  4.31it/s]2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 7/29 chunk 829b34d8-4227-435d-93d5-b699a56fbfab into collection demo\u001b[0m\n",
      "2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 8/29 chunk ea43c5c8-5e28-47d0-bd84-eacf4e120673 into collection demo\u001b[0m\n",
      "8it [00:02,  6.11it/s]2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 9/29 chunk c9fd8cb8-5798-4ef9-b6df-f79f07d7e038 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 10/29 chunk e8f445f6-e320-4752-b62f-6eea80330910 into collection demo\u001b[0m\n",
      "10it [00:02,  7.82it/s]2024-02-22 20:55:19 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 11/29 chunk 0018f2ac-f553-4d70-9f51-ba6227f50697 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 12/29 chunk a7ad4c59-e5c2-4be1-acf3-a3e18db0747c into collection demo\u001b[0m\n",
      "12it [00:02,  9.08it/s]2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 13/29 chunk 3ca34b04-b0d6-425f-8811-426e0067225b into collection demo\u001b[0m\n",
      "2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 14/29 chunk 4a7c6214-36e6-40f8-94b3-ebc70496e38d into collection demo\u001b[0m\n",
      "14it [00:02,  9.45it/s]2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 15/29 chunk a3018088-cd3c-4890-945d-a2f1997d7f56 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 16/29 chunk e9f4b4f1-4291-42d4-8c9e-4e51ead578fa into collection demo\u001b[0m\n",
      "16it [00:02,  9.73it/s]2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 17/29 chunk 86ee1092-67d7-41e1-a65a-19f8816e329c into collection demo\u001b[0m\n",
      "2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 18/29 chunk c2ea5b4b-7c5e-4c27-8f03-04a39683ce71 into collection demo\u001b[0m\n",
      "18it [00:03,  9.95it/s]2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 19/29 chunk f52d97c4-19eb-4ca6-8829-3369fcef7677 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 20/29 chunk d5eb79ae-fed8-478b-a978-c3fd23f30559 into collection demo\u001b[0m\n",
      "20it [00:03, 10.68it/s]2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 21/29 chunk 48a4482c-2330-41b1-b255-5d40a311a087 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:20 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 22/29 chunk 305e8dee-45e5-444d-a2fe-60fbf5575ef6 into collection demo\u001b[0m\n",
      "22it [00:03, 10.92it/s]2024-02-22 20:55:21 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 23/29 chunk 3843666a-69fa-464d-9406-140862d4aa04 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:21 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 24/29 chunk aad63a9e-dffb-43b5-bbf3-d55b5e479245 into collection demo\u001b[0m\n",
      "24it [00:03, 11.98it/s]2024-02-22 20:55:21 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 25/29 chunk d0c70a52-6f14-4d83-a110-fd3368976e2b into collection demo\u001b[0m\n",
      "2024-02-22 20:55:21 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 26/29 chunk 8e2fc393-6958-48be-9083-90bda1f4ae1e into collection demo\u001b[0m\n",
      "26it [00:03, 12.28it/s]2024-02-22 20:55:21 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 27/29 chunk 19cb807b-1cca-4ad1-b29e-c3e1304f239b into collection demo\u001b[0m\n",
      "2024-02-22 20:55:21 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 28/29 chunk 0ed50a23-be1e-4655-927f-591c53009d6e into collection demo\u001b[0m\n",
      "28it [00:03, 12.12it/s]2024-02-22 20:55:21 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 29/29 chunk 4ea38202-036b-41ce-8b8a-47e07420a9b1 into collection demo\u001b[0m\n",
      "29it [00:03,  7.30it/s]\n",
      "2024-02-22 20:55:21 INFO gai.gen.rag.RAG:\u001b[32mRAG.index_async: indexing...done\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Index a text file\n",
    "\n",
    "path=\"./pm_long_speech_2023.txt\"\n",
    "rag.load()\n",
    "doc_id = await rag.index_async(\n",
    "    collection_name='demo',\n",
    "    file_path=path,\n",
    "    file_type='txt',\n",
    "    source=\"https://www.pmo.gov.sg/Newsroom/2023-National-Day-Rally-Speech\",\n",
    "    title=\"2023 National Day Rally Speech\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ByteSize': 43352,\n",
      " 'ChunkSize': 2000,\n",
      " 'Chunks': 29,\n",
      " 'Collection': 'demo',\n",
      " 'File': 'txt',\n",
      " 'FileName': 'pm_long_speech_2023.txt',\n",
      " 'Id': '3f047a5665ea01fc239fce1933a75e3ec2d64e574fbfc70d4b974449896c6365',\n",
      " 'Source': 'https://www.pmo.gov.sg/Newsroom/2023-National-Day-Rally-Speech',\n",
      " 'Title': '2023 National Day Rally Speech'}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: View doc summary\n",
    "from pprint import pprint\n",
    "doc = rag.get_document(doc_id)\n",
    "pprint({\n",
    "    \"Id\":doc.Id,\n",
    "    \"Title\":doc.Title,\n",
    "    \"FileName\":doc.FileName,\n",
    "    \"File\":doc.FileType,\n",
    "    \"Source\":doc.Source,\n",
    "    \"ByteSize\":doc.ByteSize,\n",
    "    \"Collection\":doc.CollectionName,\n",
    "    \"ChunkSize\":doc.ChunkGroups[0].ChunkSize,\n",
    "    \"Chunks\": len(doc.ChunkGroups[0].Chunks)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 20:55:30 INFO gai.gen.rag.RAG:\u001b[32mRetrieving by query Who are the young seniors?...\u001b[0m\n",
      "2024-02-22 20:55:30 INFO gai.gen.rag.dalc.RAGVSRepository:\u001b[32mRetrieving by query Who are the young seniors?...\u001b[0m\n",
      "2024-02-22 20:55:31 DEBUG gai.gen.rag.dalc.RAGVSRepository:\u001b[35mresult=[['3843666a-69fa-464d-9406-140862d4aa04', 'e9f4b4f1-4291-42d4-8c9e-4e51ead578fa', 'ea43c5c8-5e28-47d0-bd84-eacf4e120673']]\u001b[0m\n",
      "2024-02-22 20:55:31 DEBUG gai.gen.rag.RAG:\u001b[35mresult={\"documents\":{\"0\":\"The seniors looked happy, but some of them were not so well. A few were wheelchair-bound, but they still joined in the activities. This cheerful lady told me she hoped to joget again! Why not, even in a wheelchair? Other seniors were using the health services at the AAC. Some were getting their vital signs checked so that doctors could follow up if something was amiss. One was having a teleconsultation \\u2013 with nurses physically there to help him, and a doctor calling in on Zoom from the polyclinic. It was good that relatives, or neighbours, were making the effort to bring the seniors down to the AAC, for them to socialise and cheer up their lives.\",\"1\":\"What I found most encouraging was that many seniors were not just taking part in activities, but also helping to organise and run them \\u2013 seniors for seniors. I met Mdm Goh, the one in the pink blouse, Mdm Karen, and Mdm Farida. They were preparing to deliver hot meals to frail seniors living in upstairs in the rental flats who could not come down to the AAC. They do this every day. It is a great way to build neighbourly bonds while staying active. Actually, besides meal deliveries, there are also communal meals downstairs at the AAC. The staff told me that these meals were a big draw and an important activity because they encourage the old folks to come down. When they gathered at the AACs for meals, they would socialise and make friends, and the staff could keep an eye on them, made sure they were alright. I also discovered that these meals are actually cooked by the seniors themselves, at least some of the seniors themselves! These three makciks \\u2013 Mdm Aminah on the left, Mdm Rosnah, and Mdm Fatimah \\u2013 they used their SkillsFuture credits to attend a food hygiene and preparation course so that they could cook meals for their fellow seniors. Mdm Fatimah told me they use less salt and sugar in their cooking to make it a Healthier Choice. Then I tasted her chicken curry, and I said, \\\"You must have used santan \\u2013 coconut milk.\\u201d She said no \\u2013 she uses normal milk. But it tasted just as good \\u2013 sedap sekali! I had a second helping.\\n\\nIf you are observant, you would have noticed that the volunteers I have mentioned are all ladies. But we are starting to see more male seniors coming down to the AACs. The centres are introducing more activities that interest men, such as jamming. Or even something simple like a caf\\u00e9 corner, where the uncles can catch up over a coffee. There will be something for everybody at the AACs.\",\"2\":\"SECTION 3: AGEING\\nI want to talk about two other important issues tonight \\u2013 caring for our ageing population, and housing our people.\\n\\nSingapore is one of the fastest-ageing nations in the world. Today, about one in five Singaporeans is a senior, aged 65 and above. By 2030, nearly one in four Singaporeans will be a senior. I first talked about ageing in the National Day Rally back in 2007. At that time, we had 500 centenarians \\u2013 people aged 100 and older. We thought that was a lot. By 2013, which is 6 years later, this had doubled to around 1,000. And by 2030, we are likely to double again to at least 2,000 centenarians! It shows vividly how our society is getting older and older, faster and faster. Today, if you ask \\u2013 it is not in the chart \\u2013 but we have about 1,500 already. In 2030, I will not be making this speech, but whoever is doing so will have to take care of all of us seniors, and some of us very senior.\\n\\nToday, we are an aged society; soon, we will be a \\u2018super-aged\\u2019 society. This has massive social and economic implications. We have much to do to help our seniors age well.\\n\\nToday, I will speak on two aspects of our preparations: active ageing; and making homes and precincts more senior-friendly.\\n\\nActive Ageing\\nFirst, on active ageing.\\n\\nWe have invested significantly in healthcare for seniors.\"},\"metadatas\":{\"0\":{\"Abstract\":\"\",\"ChunkGroupId\":\"34ddec89-75ee-49a4-a265-c2d2ee971626\",\"DocumentId\":\"3f047a5665ea01fc239fce1933a75e3ec2d64e574fbfc70d4b974449896c6365\",\"Keywords\":\"\",\"PublishedDate\":\"\",\"Source\":\"https:\\/\\/www.pmo.gov.sg\\/Newsroom\\/2023-National-Day-Rally-Speech\",\"Title\":\"2023 National Day Rally Speech\"},\"1\":{\"Abstract\":\"\",\"ChunkGroupId\":\"34ddec89-75ee-49a4-a265-c2d2ee971626\",\"DocumentId\":\"3f047a5665ea01fc239fce1933a75e3ec2d64e574fbfc70d4b974449896c6365\",\"Keywords\":\"\",\"PublishedDate\":\"\",\"Source\":\"https:\\/\\/www.pmo.gov.sg\\/Newsroom\\/2023-National-Day-Rally-Speech\",\"Title\":\"2023 National Day Rally Speech\"},\"2\":{\"Abstract\":\"\",\"ChunkGroupId\":\"34ddec89-75ee-49a4-a265-c2d2ee971626\",\"DocumentId\":\"3f047a5665ea01fc239fce1933a75e3ec2d64e574fbfc70d4b974449896c6365\",\"Keywords\":\"\",\"PublishedDate\":\"\",\"Source\":\"https:\\/\\/www.pmo.gov.sg\\/Newsroom\\/2023-National-Day-Rally-Speech\",\"Title\":\"2023 National Day Rally Speech\"}},\"distances\":{\"0\":0.1210176349,\"1\":0.123644352,\"2\":0.127476573},\"ids\":{\"0\":\"3843666a-69fa-464d-9406-140862d4aa04\",\"1\":\"e9f4b4f1-4291-42d4-8c9e-4e51ead578fa\",\"2\":\"ea43c5c8-5e28-47d0-bd84-eacf4e120673\"}}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>metadatas</th>\n",
       "      <th>distances</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstract</th>\n",
       "      <td>The seniors looked happy, but some of them wer...</td>\n",
       "      <td></td>\n",
       "      <td>0.121018</td>\n",
       "      <td>3843666a-69fa-464d-9406-140862d4aa04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  documents metadatas  \\\n",
       "Abstract  The seniors looked happy, but some of them wer...             \n",
       "\n",
       "          distances                                   ids  \n",
       "Abstract   0.121018  3843666a-69fa-464d-9406-140862d4aa04  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Retrieve answers\n",
    "rag.retrieve(collection_name=\"demo\",query_texts=\"Who are the young seniors?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 487.99it/s]\n",
      "2024-02-22 20:55:42 INFO gai.gen.rag.RAG:\u001b[32mRAG.index_async: Begin indexing...\u001b[0m\n",
      "0it [00:00, ?it/s]2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 1/22 chunk dacf656c-b424-4a0f-b192-40ab630bc84c into collection demo\u001b[0m\n",
      "1it [00:00,  1.88it/s]2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 2/22 chunk d82f6aa6-3974-46fb-8107-691e7cc6c5dc into collection demo\u001b[0m\n",
      "2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 3/22 chunk 889ab410-3a3f-4881-8e60-23f51483adf9 into collection demo\u001b[0m\n",
      "3it [00:00,  4.78it/s]2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 4/22 chunk 0d343170-c4e1-42bc-8e05-768af1fbf882 into collection demo\u001b[0m\n",
      "4it [00:00,  5.68it/s]2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 5/22 chunk a9394ed3-afbd-4d64-95f7-540396de2c57 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 6/22 chunk da98e055-40d4-4efd-ad13-4fa26b4184d8 into collection demo\u001b[0m\n",
      "6it [00:00,  8.00it/s]2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 7/22 chunk afa32cda-a393-4bde-81d7-18d00e15c364 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:43 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 8/22 chunk d3fedc6b-eb3a-4de1-8294-ad97057f9329 into collection demo\u001b[0m\n",
      "8it [00:01,  8.64it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 9/22 chunk 79c41cc5-e59c-423f-a787-32eaf5702f73 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 10/22 chunk 56e441b1-d822-4e96-9dfa-ceedbf36c678 into collection demo\u001b[0m\n",
      "10it [00:01,  9.06it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 11/22 chunk 8ae3a0d1-7de7-4acf-aeab-21d400ccfb92 into collection demo\u001b[0m\n",
      "11it [00:01,  9.14it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 12/22 chunk 4b2f2a3f-f4ef-4771-99b9-71d31261ee2c into collection demo\u001b[0m\n",
      "12it [00:01,  9.10it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 13/22 chunk ba30e496-36ec-468a-a9c3-a55fc5df030d into collection demo\u001b[0m\n",
      "13it [00:01,  9.18it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 14/22 chunk c97f1d8c-0496-46e4-b27c-10519eb38092 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 15/22 chunk bd781aa1-3815-4d96-a454-705014802993 into collection demo\u001b[0m\n",
      "15it [00:01,  9.44it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 16/22 chunk d126f36e-7967-4a87-ae37-77fcfce87646 into collection demo\u001b[0m\n",
      "16it [00:02,  9.33it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 17/22 chunk e83b2ce3-3cdc-4dfa-8e25-8cfca95ad12d into collection demo\u001b[0m\n",
      "17it [00:02,  9.34it/s]2024-02-22 20:55:44 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 18/22 chunk 8fc9677e-2dca-4e1a-a805-c44142beaa9d into collection demo\u001b[0m\n",
      "18it [00:02,  9.25it/s]2024-02-22 20:55:45 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 19/22 chunk 72549914-03f7-4bee-84d1-60ff021aae64 into collection demo\u001b[0m\n",
      "19it [00:02,  9.23it/s]2024-02-22 20:55:45 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 20/22 chunk 379a82d1-3e64-4b88-9f1a-f6b03411f49c into collection demo\u001b[0m\n",
      "20it [00:02,  9.33it/s]2024-02-22 20:55:45 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 21/22 chunk c1bf71f9-21af-4f02-af5a-a492147de436 into collection demo\u001b[0m\n",
      "2024-02-22 20:55:45 DEBUG gai.gen.rag.RAG:\u001b[35mRAG.index_async: Indexed 22/22 chunk bc42f40b-1e70-45dd-b536-ba7420bc8884 into collection demo\u001b[0m\n",
      "22it [00:02,  8.28it/s]\n",
      "2024-02-22 20:55:45 INFO gai.gen.rag.RAG:\u001b[32mRAG.index_async: indexing...done\u001b[0m\n",
      "2024-02-22 20:55:45 INFO gai.gen.rag.RAG:\u001b[32mRetrieving by query How is the transformer different from RNN?...\u001b[0m\n",
      "2024-02-22 20:55:45 INFO gai.gen.rag.dalc.RAGVSRepository:\u001b[32mRetrieving by query How is the transformer different from RNN?...\u001b[0m\n",
      "2024-02-22 20:55:45 DEBUG gai.gen.rag.dalc.RAGVSRepository:\u001b[35mresult=[['da98e055-40d4-4efd-ad13-4fa26b4184d8', 'c1bf71f9-21af-4f02-af5a-a492147de436', 'd82f6aa6-3974-46fb-8107-691e7cc6c5dc']]\u001b[0m\n",
      "2024-02-22 20:55:45 DEBUG gai.gen.rag.RAG:\u001b[35mresult={\"documents\":{\"0\":\"This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with\",\"1\":\"In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\",\"2\":\"data. \\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. \\u2020Work performed while at Google Brain. \\u2021Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. 1 Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht\\u22121 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent\"},\"metadatas\":{\"0\":{\"Abstract\":\"\",\"ChunkGroupId\":\"c9aa489f-1aa1-4714-a5c0-a62543003d72\",\"DocumentId\":\"f9273d797cd489295a155dea10368a6a6df70686a692fbf8b5e6bc60fb23a72d\",\"Keywords\":\"\",\"PublishedDate\":\"\",\"Source\":\"arxiv.org\",\"Title\":\"Attention is All You Need\"},\"1\":{\"Abstract\":\"\",\"ChunkGroupId\":\"c9aa489f-1aa1-4714-a5c0-a62543003d72\",\"DocumentId\":\"f9273d797cd489295a155dea10368a6a6df70686a692fbf8b5e6bc60fb23a72d\",\"Keywords\":\"\",\"PublishedDate\":\"\",\"Source\":\"arxiv.org\",\"Title\":\"Attention is All You Need\"},\"2\":{\"Abstract\":\"\",\"ChunkGroupId\":\"c9aa489f-1aa1-4714-a5c0-a62543003d72\",\"DocumentId\":\"f9273d797cd489295a155dea10368a6a6df70686a692fbf8b5e6bc60fb23a72d\",\"Keywords\":\"\",\"PublishedDate\":\"\",\"Source\":\"arxiv.org\",\"Title\":\"Attention is All You Need\"}},\"distances\":{\"0\":0.1278051734,\"1\":0.1321320534,\"2\":0.1348036528},\"ids\":{\"0\":\"da98e055-40d4-4efd-ad13-4fa26b4184d8\",\"1\":\"c1bf71f9-21af-4f02-af5a-a492147de436\",\"2\":\"d82f6aa6-3974-46fb-8107-691e7cc6c5dc\"}}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>metadatas</th>\n",
       "      <th>distances</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstract</th>\n",
       "      <td>This inherently sequential nature precludes pa...</td>\n",
       "      <td></td>\n",
       "      <td>0.127805</td>\n",
       "      <td>da98e055-40d4-4efd-ad13-4fa26b4184d8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  documents metadatas  \\\n",
       "Abstract  This inherently sequential nature precludes pa...             \n",
       "\n",
       "          distances                                   ids  \n",
       "Abstract   0.127805  da98e055-40d4-4efd-ad13-4fa26b4184d8  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index and Retrieve PDF\n",
    "path = \"./attention-is-all-you-need.pdf\"\n",
    "rag.unload()\n",
    "rag.load()\n",
    "doc_id = await rag.index_async(\n",
    "    collection_name='demo',\n",
    "    file_path=path,\n",
    "    file_type='pdf',\n",
    "    source=\"arxiv.org\",\n",
    "    title=\"Attention is All You Need\",\n",
    "    )\n",
    "rag.retrieve(collection_name=\"demo\",query_texts=\"How is the transformer different from RNN?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Running as a Service\n",
    "\n",
    "In this example, we will start 2 services: one for RAG API and one for RAG Listener.\n",
    "We will then index a document using curl and observe the progress using the listener.\n",
    "\n",
    "### Step 1: Start the API service\n",
    "\n",
    "#### Option A: Run in a Docker container (Recommended)\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "    --name gai-rag \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-rag:latest\n",
    "```\n",
    "\n",
    "Wait for model to load\n",
    "\n",
    "```bash\n",
    "docker logs gai-rag\n",
    "```\n",
    "\n",
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "#### Option B: Run from Terminal\n",
    "\n",
    "```bash\n",
    "cd /gai-gen/gai/api/\n",
    "python rag_api.py\n",
    "```\n",
    "\n",
    "### Step 2: Start the Listener Service\n",
    "\n",
    "The listener can be helpful when used with the API. It can be used to monitor the indexing progress via web socket. \n",
    "This is especially useful when monitoring the progress while indexing large files.\n",
    "\n",
    "```python\n",
    "# prettier-ignore\n",
    "import asyncio\n",
    "import os, sys\n",
    "import websockets\n",
    "\n",
    "async def listen():\n",
    "    ws_uri = \"ws://localhost:12031/api/v1/rag/index-file/ws\"\n",
    "    async with websockets.connect(ws_uri) as websocket:\n",
    "        while True:\n",
    "            message = await websocket.recv()\n",
    "            logger.info(f\"Received: {message}\")\n",
    "\n",
    "asyncio.run(listen())\n",
    "```\n",
    "\n",
    "The above code is saved under `/tests/integration_tests/rag/rag_listener`.\n",
    "\n",
    "```bash\n",
    "cd tests/integration_tests/rag\n",
    "python rag_listener.py\n",
    "```\n",
    "\n",
    "If the listener is successfully started, you should see the following message from the API Server logs:\n",
    "\n",
    "![rag-listener-connected](./imgs/rag-listener-connected.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test RAG\n",
    "\n",
    "**Send Request**\n",
    "\n",
    "```bash\n",
    "cd tests/integration_tests/rag\n",
    "```\n",
    "\n",
    "The following example uses curl script `tests/integration_tests/rag/3_curl_index.sh` to index a file .\n",
    "\n",
    "```bash\n",
    "curl -X POST 'http://localhost:12031/gen/v1/rag/index-file' \\\n",
    "    -H 'accept: application/json' \\\n",
    "    -H 'Content-Type: multipart/form-data' \\\n",
    "    -s \\\n",
    "    -F 'collection_name=demo' \\\n",
    "    -F 'file=@./pm_long_speech_2023.txt' \\\n",
    "    -F 'metadata={\"source\": \"https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2023#:~:text=COVID%2D19%20was%20the%20most,indomitable%20spirit%20of%20our%20nation.\"}'\n",
    "```\n",
    "\n",
    "**NOTE**: The indexing may fail if the file was already indexed. To re-index, you can delete the demo collection.\n",
    "\n",
    "```bash\n",
    "curl -X DELETE 'http://localhost:12031/gen/v1/rag/collection/demo'\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n",
    "\n",
    "![gai-gen-rag](../doc/docs/gai-gen/imgs/gai-gen-rag.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
