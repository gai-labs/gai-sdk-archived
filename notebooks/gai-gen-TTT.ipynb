{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Text (TTT)\n",
    "\n",
    "## 1.1 Setting Up\n",
    "\n",
    "We will create a seperate virtual environment for this to avoid conflicting dependencies that each underlying model requires.\n",
    "\n",
    "```sh\n",
    "sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "conda create -n TTT python=3.10.10 -y\n",
    "conda activate TTT\n",
    "pip install -e \".[TTT]\"\n",
    "```\n",
    "\n",
    "The following examples has been tested on the following environment:\n",
    "\n",
    "-   NVidia GeForce RTX 2060 6GB\n",
    "-   Windows 11 + WSL2\n",
    "-   Ubuntu 22.04\n",
    "-   Python 3.10\n",
    "-   CUDA Toolkit 11.8\n",
    "-   openai 1.6.1\n",
    "-   anthropic 0.8.1\n",
    "-   transformers 4.36.2\n",
    "-   bitsandbytes 0.41.3.post2\n",
    "-   scipy 1.11.4\n",
    "-   accelerate 0.25.0\n",
    "-   llama-cpp-python 0.2.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Running as a Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT4\n",
    "\n",
    "For (1) and (2) below, you will use the GaiGen library to call OpenAI's GPT4.\n",
    "You will need to get an API key from OpenAI. \n",
    "Create .env file in project root directory and insert the OpenAI API Key below:\n",
    "\n",
    "```sh\n",
    "OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:36:39 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator gpt-4...\u001b[0m\n",
      "2024-04-24 00:36:39 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine OpenAI_TTT...\u001b[0m\n",
      "2024-04-24 00:36:39 DEBUG gai.gen.ttt.OpenAI_TTT:\u001b[35mOpenAI_TTT.create: model_params={'max_tokens': 100, 'temperature': 0.7, 'top_p': 1, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop': None, 'logit_bias': {}, 'n': 1}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a small, tranquil seaside town, there lived a humble fisherman named Eli. Every day, Eli would cast his nets into the ocean, patiently waiting for his bounty, his livelihood. One fateful afternoon, as a golden sunset painted the sky, he pulled up an old, tarnished bottle from the depths. As he uncorked it, a magical genie appeared offering him three wishes. Instead of asking for wealth or power, Eli wished for the ocean to be forever\n"
     ]
    }
   ],
   "source": [
    "### 1. GPT4 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}], max_tokens=100,stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:36:52 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: Generator is already loaded. Skip loading.\u001b[0m\n",
      "2024-04-24 00:36:52 DEBUG gai.gen.ttt.OpenAI_TTT:\u001b[35mOpenAI_TTT.create: model_params={'max_tokens': 100, 'temperature': 0.7, 'top_p': 1, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'stop': None, 'logit_bias': {}, 'n': 1}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      "Once upon a time in a small, tranquil village lived a humble cobbler named Sam. Sam was known for his extraordinary craftsmanship and a heart full of kindness. One day, a frail, old woman visited him with a pair of worn-out shoes and a sorrowful story of her inability to afford new ones. Touched by her story, Sam worked all night to mend her shoes, turning them into a pair as splendid as new. The following morning, the woman returned to find her transformed shoes"
     ]
    }
   ],
   "source": [
    "### 2. GPT4 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],stream=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B 8k-context 4-bit quantized\n",
    "\n",
    "For (3) and (4), you will run Mistral 7B locally. Clone TheBloke's 4-bit quantized version of Mistral-7B model from hugging face. This model utilizes the exLlama loader for increased performance. Make sure you have huggingface-hub installed, if not run `pip install huggingface-hub`.\n",
    "\n",
    "```sh\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        config.json\n",
    "        model.safetensors \n",
    "        tokenizer.model\n",
    "        --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ \\\n",
    "        --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:37:03 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: New generator_name specified, unload current generator.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:37:11 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator mistral7b-exllama...\u001b[0m\n",
      "2024-04-24 00:37:11 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine ExLlama_TTT...\u001b[0m\n",
      "2024-04-24 00:37:11 INFO gai.gen.ttt.TTT:\u001b[32mLoading model from models/Mistral-7B-Instruct-v0.1-GPTQ\u001b[0m\n",
      "2024-04-24 00:37:11 INFO gai.gen.ttt.ExLlama_TTT:\u001b[32mExLlama_TTT.load: Loading model from /home/roylai/gai/models/Mistral-7B-Instruct-v0.1-GPTQ/model.safetensors\u001b[0m\n",
      "2024-04-24 00:37:25 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: prompt=USER: Tell me a one paragraph short story.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-04-24 00:37:25 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-04-24 00:37:25 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: input token count=16\u001b[0m\n",
      "2024-04-24 00:37:31 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: stopped by max_new_tokens: 100\u001b[0m\n",
      "2024-04-24 00:37:31 INFO gai.gen.ttt.ExLlama_TTT:\u001b[32mExLlama_TTT._generating: output= Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who was known for her wisdom and kindness. She had spent her entire life studying the mysteries of nature and the secrets of the universe, and she believed that everything happened for a reason. One day, as she sat on her porch watching the sun set over the mountains, she noticed a young boy playing in the field across the street chunk_type=text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who was known for her wisdom and kindness. She had spent her entire life studying the mysteries of nature and the secrets of the universe, and she believed that everything happened for a reason. One day, as she sat on her porch watching the sun set over the mountains, she noticed a young boy playing in the field across the street\n"
     ]
    }
   ],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100, stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:37:39 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: Generator is already loaded. Skip loading.\u001b[0m\n",
      "2024-04-24 00:37:39 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: prompt=USER: Tell me a one paragraph short story.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-04-24 00:37:39 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-04-24 00:37:39 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: input token count=16\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING:\n",
      " Once upon a time, in a small village nestled at the foot of a mountain, there lived an old woman who was known for her wisdom and kindness. She had spent her entire life studying the mysteries of nature and the secrets of the universe, and she had gained a deep understanding of the interconnectedness of all things. One day, as she sat by the riverbank, watching the"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:37:42 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: stopped by max_new_tokens: 100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sun set over the mountains, she felt a sense"
     ]
    }
   ],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yarn-Mistral-7B 128k-context 4-bit quantized\n",
    "\n",
    "Repeat the earlier examples but using a different version of Mistral-7B model with a larger context window.\n",
    "\n",
    "```sh\n",
    "huggingface-cli download TheBloke/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir ~/gai/models/Yarn-Mistral-7B-128k-GPTQ \\\n",
    "        --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to their paper, the perplexity seems better than the original once the token length is greater than 10k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perplexity-of-mistral7b-128k](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:37:49 DEBUG gai.gen.Gaigen:\u001b[35mGaigen.load: New generator_name specified, unload current generator.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 00:37:50 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator mistral7b_128k-exllama...\u001b[0m\n",
      "2024-04-24 00:37:50 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine ExLlama_TTT...\u001b[0m\n",
      "2024-04-24 00:37:50 INFO gai.gen.ttt.TTT:\u001b[32mLoading model from models/Yarn-Mistral-7B-128k-GPTQ\u001b[0m\n",
      "2024-04-24 00:37:50 INFO gai.gen.ttt.ExLlama_TTT:\u001b[32mExLlama_TTT.load: Loading model from /home/roylai/gai/models/Yarn-Mistral-7B-128k-GPTQ/model.safetensors\u001b[0m\n",
      "2024-04-24 00:38:12 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: prompt=USER: Tell me a one paragraph short story.\n",
      "ASSISTANT:\u001b[0m\n",
      "2024-04-24 00:38:12 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: model_params={'temperature': 1.2, 'top_p': 0.15, 'min_p': 0.0, 'top_k': 50, 'max_new_tokens': 100, 'typical': 0.0, 'token_repetition_penalty_max': 1.25, 'token_repetition_penalty_sustain': 256, 'token_repetition_penalty_decay': 128, 'beams': 1, 'beam_length': 1}\u001b[0m\n",
      "2024-04-24 00:38:12 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: input token count=16\u001b[0m\n",
      "2024-04-24 00:38:28 DEBUG gai.gen.ttt.ExLlama_TTT:\u001b[35mExLlama_TTT._streaming: stopped by max_new_tokens: 100\u001b[0m\n",
      "2024-04-24 00:38:28 INFO gai.gen.ttt.ExLlama_TTT:\u001b[32mExLlama_TTT._generating: output= Once upon a time, there was a little girl who loved to play outside in the sunshine. One day she went out into her backyard and saw something strange – it looked like a giant spider web! She walked closer and realized that it wasn’t a spider at all but rather an intricate pattern of leaves on the ground. The more she explored this beautiful design, the happier she became until finally she sat down under its chunk_type=text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, there was a little girl who loved to play outside in the sunshine. One day she went out into her backyard and saw something strange – it looked like a giant spider web! She walked closer and realized that it wasn’t a spider at all but rather an intricate pattern of leaves on the ground. The more she explored this beautiful design, the happier she became until finally she sat down under its"
     ]
    }
   ],
   "source": [
    "### 3. Mistral Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Mistral Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b_128k-exllama')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    if (chunk.choices[0].delta.content):\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropics Claude2.1\n",
    "\n",
    "The following example uses Anthropics Claude2.1 100k context window size model. Get API Key from Anthropics and add it to the .env file.\n",
    "```sh\n",
    "ANTHROPIC_APIKEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Claude-2.1 Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Claude-2.1 Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('claude2-100k')\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_tokens_to_sample=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B with HuggingFace transformers\n",
    "\n",
    "Follow the instructions [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama2) to signup with Meta to download the LLaMa-2 model.\n",
    "Download the model in HuggingFace format from [here] (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) into ~/gai/models/Llama-2-7b-chat-hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Llama2-7B Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Llama2-7B Text-to-Text Streaming\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama2-transformers')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 7B GGUF with LlaMaCPP (CPU only)\n",
    "\n",
    "The following example uses GGUF formatted version of Mistral-7B for LlaMaCPP. This can be used when you want the model to run off CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download the model\n",
    "huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                mistral-7b-instruct-v0.1.Q4_K_M.gguf  \\\n",
    "                config.json \\\n",
    "                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GGUF \\\n",
    "                --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"GENERATING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "import sys\n",
    "with io.capture_output() as captured:\n",
    "    # Redirect stderr to stdout\n",
    "    sys.stderr = sys.stdout    \n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100)\n",
    "print(response.choices[0].message.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Mistral-7B CPU-Only Text-to-Text Generation\n",
    "\n",
    "print(\"STREAMING:\")\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}],max_new_tokens=100,stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end='',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Using Function Call\n",
    "\n",
    "OpenAPI provided a powerful feature for its API called Function calling. It is essentially a way for the LLM to seek external help when encountering limitation to its ability to generate text but returning a string emulating the calling of a function based on the function description provied by the user.\n",
    "\n",
    "We will create a set of tools that can be made available to the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"gg\",\n",
    "            \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"scrape\",\n",
    "            \"description\": \"Scrape the content of the provided url\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The url to scrape the content from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will show how it works in gpt-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('gpt-4')\n",
    "\n",
    "highlight(\"Model decided to use tool: \")\n",
    "user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n",
    "\n",
    "highlight(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we demonstrate the same feature applied to Mistral-7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.gen import Gaigen\n",
    "from gai.common.notebook import highlight\n",
    "gen = Gaigen.GetInstance().load('mistral7b-exllama')\n",
    "\n",
    "highlight(\"Model decided to use tool: \")\n",
    "user_prompt = \"Where did PM Lee Hsien Loong hold his 2023 national day rally?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n",
    "\n",
    "highlight(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Returning JSON\n",
    "\n",
    "Here's an example of how to return a JSON output. This is useful for returning structured from unstructured result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "The user will show you information scraped from the web. \n",
    "You will analyse and return a <relevance> format.\n",
    "\n",
    "DEFINITIONS:\n",
    "1. A <relevance> response is based on the following JSON format:\n",
    "        <relevance>\n",
    "        {\n",
    "            'type': 'json',\n",
    "            'json': {\n",
    "                'Relevance': '<relevance>%',\n",
    "                'Reason': '<reason>'\n",
    "            }\n",
    "        }\n",
    "        </relevance>\n",
    "\n",
    "You have a tendency to forget the question while you are analysing the answer. \n",
    "Follow these steps:\n",
    "1. Break down the answer into smaller chunks.\n",
    "2. Ask yourself what is the question again then assess whether any part of the information within the answer is relevant to any part of the question.\n",
    "3. Finally, you give a percentage overall score of <relevance> immediately. If it contains any direct relevance, you must give a score of at least 40%.\n",
    "You will not be given a second chance or any further context or information. If you think the answer is not related to the question, just reply with \"Not relevant\" and give a reason.\n",
    "Rule:\n",
    "a. <relevance> is a string between \"0%\" and \"100%\". If Not relevant, return \"0\".\n",
    "b. Pay attention to small part of the answer that addresses the question directly. Even when it is small part, the direct relevance is highly significant.\n",
    "c. Respond only with <relevence> response and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "My question is 'Where is PM Lee Hsien Loong 2023 National Day Rally location?'.\n",
    "Refer to this <webpage>Once upon a time, there was a little girl named Alice who lived in an enchanted forest full of talking animals and magical creatures. One day, she stumbled upon a secret door hidden behind a waterfall and decided to explore it. As she entered through the door, she found herself in a beautiful kingdom ruled by a kind queen. The queen welcomed her warmly and showed her around the castle, introducing her to all sorts of fascinating people and things. Alice had so much fun that she never wanted to leave!</webpage> and return a <relevance> response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'system','content':system_prompt},\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        stream=True,\n",
    "        max_new_tokens=200)\n",
    "for output in response:\n",
    "    if output.choices[0].delta.content:\n",
    "        print(output.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Running as a Service\n",
    "\n",
    "### Step 1: Start Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Stop any container with the same name\n",
    "docker rm -f gai-ttt\n",
    "\n",
    "# Start the container\n",
    "docker run -d \\\n",
    "    --name gai-ttt \\\n",
    "    -p 12031:12031 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    kakkoii1337/gai-ttt:latest\n",
    "\n",
    "# Wait for model to load\n",
    "sleep 30\n",
    "\n",
    "# Confirm its running\n",
    "docker logs gai-ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the loading is completed, the logs should show this:\n",
    "\n",
    "```bash\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:12031 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "### Step 2: Run Text Generation Client\n",
    "\n",
    "The default model is Mistral7B-8k context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a one paragraph short story.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": True\n",
    "    },\n",
    "    stream=True)\n",
    "for chunk in response.iter_lines():\n",
    "    result = json.loads(chunk.decode('utf-8'))\n",
    "    print(result[\"choices\"][0][\"delta\"][\"content\"],end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,json\n",
    "response = requests.post(\n",
    "    url='http://localhost:12031/gen/v1/chat/completions', \n",
    "    json={\n",
    "        \"model\": \"mistral7b-exllama\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is today's date?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"stream\": False,\n",
    "        \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    stream=False)\n",
    "\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
