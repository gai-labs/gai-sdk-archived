{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Gen: Text-to-Text Generation using Gai with LlamaCPP\n",
    "\n",
    "This is useful for running LLM on CPU without relying on graphics card. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "1. Create a conda environment called `TTT`, if not already created, and install the dependencies:\n",
    "\n",
    "    ```bash\n",
    "    sudo apt update -y && sudo apt install ffmpeg git git-lfs -y\n",
    "    conda create -n TTT python=3.10.10 -y\n",
    "    conda activate TTT\n",
    "    cd ../../gai-gen\n",
    "    pip install -e \".[TTT]\"\n",
    "    ```\n",
    "\n",
    "2. Download Llama3_8b GGUF model into `~/gai/models` directory.\n",
    "\n",
    "    ```bash\n",
    "    huggingface-cli download bartowski/LLaMA3-iterative-DPO-final-GGUF \\\n",
    "                    LLaMA3-iterative-DPO-final-Q4_K_M.gguf  \\\n",
    "                    --local-dir ~/gai/models/LLaMA3-iterative-DPO-final-GGUF \\\n",
    "                    --local-dir-use-symlinks False\n",
    "    ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 22:02:06 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator llama3-llamacpp...\u001b[0m\n",
      "2024-06-04 22:02:06 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine LlamaCpp_TTT...\u001b[0m\n",
      "2024-06-04 22:02:06 INFO gai.gen.ttt.TTT:\u001b[32mLoading model from models/LLaMA3-iterative-DPO-final-GGUF\u001b[0m\n",
      "2024-06-04 22:02:06 INFO gai.gen.ttt.LlamaCpp_TTT:\u001b[32mexllama_engine.load: Loading model from /home/roylai/gai/models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of an ancient forest, there lived a wise old owl named Hooten. One day, as he perched on his favorite branch, he noticed a tiny acorn nestled in the moss below. Curious, Hooten decided to watch over it and see what would happen. Days turned into weeks, and the acorn began to sprout. A small sapling emerged, growing taller with each passing day under Hooten's watchful eyes. As the seasons changed, the young tree flourished, eventually becoming a majestic oak, its branches reaching high for the sky. Hooten, now an old friend of the tree, would sit atop it and share stories with the forest creatures, forever grateful for the chance to witness life's cycle from acorn to oak. And so, the tale of Hooten and the Oak became a cherished legend among the woodland dwellers, reminding them all that even the smallest beginnings can grow into something extraordinary."
     ]
    }
   ],
   "source": [
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama3-llamacpp')\n",
    "\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}], max_tokens=1000,stream=True)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:10:01 INFO gai.gen.Gaigen:\u001b[32mGaigen: Loading generator llama3-llamacpp...\u001b[0m\n",
      "2024-06-04 20:10:01 INFO gai.gen.ttt.TTT:\u001b[32mUsing engine LlamaCpp_TTT...\u001b[0m\n",
      "2024-06-04 20:10:01 INFO gai.gen.ttt.TTT:\u001b[32mLoading model from models/LLaMA3-iterative-DPO-final-GGUF\u001b[0m\n",
      "2024-06-04 20:10:01 INFO gai.gen.ttt.LlamaCpp_TTT:\u001b[32mexllama_engine.load: Loading model from /home/roylai/gai/models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NO STREAMING\n",
      "In the heart of an ancient forest, there lived a wise old owl named Hooten. One day, as he perched on his favorite branch, he noticed a tiny acorn nestled in the moss below. Curious, Hooten decided to watch over it and see what would happen. Days turned into weeks, and the acorn began to sprout. A small sapling emerged, growing taller with each passing day under Hooten's watchful eyes. As the seasons changed, the young tree flourished, becoming a beacon of life in the forest. Hooten felt proud, knowing that he had played a part in nurturing this new addition to their home. And so, the tale of Hooten and the acorn became a cherished legend among the woodland creatures, reminding them all of the beauty that can bloom from even the smallest beginnings.\n"
     ]
    }
   ],
   "source": [
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama3-llamacpp')\n",
    "\n",
    "response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}], max_tokens=1000,stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Gai API Service\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build the docker image for the Gai API service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --build-arg CATEGORY=ttt -f ../../gai-gen/Dockerfile.TTT -t gai-ttt:latest ../../gai-gen    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container rm -f gai-ttt\n",
    "\n",
    "# Map model directory from host to container \n",
    "!docker run -d \\\n",
    "            -e DEFAULT_GENERATOR=llama3-llamacpp \\\n",
    "            -e OPENAI_API_KEY=${OPENAI_API_KEY} \\\n",
    "            --gpus all \\\n",
    "            -v ~/gai/models:/app/models \\\n",
    "            -p 12031:12031 \\\n",
    "            --name gai-ttt \\\n",
    "            gai-ttt:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Send a POST request to the API service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo STREAMING\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d '{\"model\":\"llama3-llamacpp\", \n",
    "        \"messages\": [ \n",
    "            {\"role\": \"user\",\"content\": \"Tell me a story\"}, \n",
    "            {\"role\": \"assistant\",\"content\": \"\"} \n",
    "        ], \"stream\":true, \"max_new_tokens\":1000 }' | python ../../gai-gen/tests/integration_tests/print_delta.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## JSON Mode\n",
    "\n",
    "1. Install PyLLMCore\n",
    "\n",
    "  ```bash\n",
    "  pip install py-llm-core\n",
    "  ```\n",
    "\n",
    "2. Generate JSON Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "\n",
    "# Define Grammar\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "from pydantic import TypeAdapter\n",
    "from llama_cpp import LlamaGrammar    \n",
    "type_adaptor=TypeAdapter(Book)\n",
    "schema=type_adaptor.json_schema()\n",
    "grammar = LlamaGrammar.from_json_schema(json.dumps(schema))\n",
    "\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('llama3-llamacpp')\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "response = gen.create(messages=[{'role':'USER','content':text},{'role':'ASSISTANT','content':''}], grammar=grammar, max_tokens=1000,stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract structured information about a long speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from gai.gen import Gaigen\n",
    "gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "from pydantic import TypeAdapter\n",
    "from llama_cpp import LlamaGrammar  \n",
    "import json  \n",
    "type_adaptor=TypeAdapter(Book)\n",
    "schema=type_adaptor.json_schema()\n",
    "grammar = LlamaGrammar.from_json_schema(json.dumps(schema))\n",
    "\n",
    "with open(\"pm_long_speech_2023.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "import os\n",
    "model = \"~/gai/models/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n",
    "model = os.path.expanduser(model)\n",
    "response = gen.create(messages=[{'role':'USER','content':text},{'role':'ASSISTANT','content':''}], grammar=grammar, max_tokens=-1,stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternate approach using PyLLMCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "from pydantic import TypeAdapter\n",
    "type_adaptor=TypeAdapter(Book)\n",
    "schema=type_adaptor.json_schema()\n",
    "\n",
    "import os\n",
    "model = \"~/gai/models/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n",
    "model = os.path.expanduser(model)\n",
    "from llm_core.parsers import LLaMACPPParser\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "\n",
    "with LLaMACPPParser(Book, model=model,llama_cpp_kwargs={\"n_ctx\":32000,\"verbose\":False}) as parser:\n",
    "    book = parser.parse(text)\n",
    "    print(book)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Function Calling\n",
    "\n",
    "It is essentially a way for the LLM to seek external help when encountering limitation to its ability to generate text but returning a string emulating the calling of a function based on the function description provied by the user.\n",
    "\n",
    "We will create a set of tools that can be made available to the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.gen import Gaigen\n",
    "#gen = Gaigen.GetInstance().load('mistral7b-llamacpp')\n",
    "gen = Gaigen.GetInstance().load('llama3-llamacpp')\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google\",\n",
    "            \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"scrape\",\n",
    "            \"description\": \"Scrape the content of the provided url\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The url to scrape the content from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "from gai.common.notebook import highlight\n",
    "\n",
    "highlight(\"Model decided to use tool: \")\n",
    "user_prompt = \"What time is it in Singapore right now?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "    tools=tools,\n",
    "    stream=False,\n",
    "    max_new_tokens=200)\n",
    "print(response.choices[0].message)\n",
    "\n",
    "highlight(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        max_new_tokens=200)\n",
    "print(response.choices[0].message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
