{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gai/Lib: Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Text (TTT)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Build the docker image using the following command in terminal.\n",
    "\n",
    "```bash\n",
    "docker buildx build \\\n",
    "    --build-arg CATEGORY=ttt2 \\\n",
    "    -f ../../gai-sdk/gai-gen/Dockerfile.torch2.2-cuda12.1-cudnn8 \\\n",
    "    -t gai-ttt:latest ../../gai-sdk/gai-gen/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: exllamav2-mistral7b (**GPU Required**)\n",
    "\n",
    "Download model\n",
    "\n",
    "```bash\n",
    "huggingface-cli download bartowski/Mistral-7B-Instruct-v0.3-exl2 \\\n",
    "    --revision 1a09a351a5fb5a356102bfca2d26507cdab11111 \\\n",
    "    --local-dir ~/gai/models/Mistral-7B-Instruct-v0.3-exl2 \\\n",
    "    --local-dir-use-symlinks False\n",
    "\n",
    "```\n",
    "\n",
    "1. Start service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm -f gai-ttt\n",
    "docker run -d \\\n",
    "    -e DEFAULT_GENERATOR=exllamav2-mistral7b \\\n",
    "    -e LOG_LEVEL=DEBUG \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    -p 12031:12031 \\\n",
    "    --name gai-ttt \\\n",
    "    gai-ttt:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"ttt\",type=\"gai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "#ggg=GGG(category=\"ttt\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "for chunk in ggg(\n",
    "    messages=\"user: Tell me a one paragraph story\\nassistant:\",\n",
    "    max_new_tokens=100,\n",
    "    max_tokens=100\n",
    "    ):\n",
    "    print(chunk.decode(),end=\"\",flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'function', 'name': 'gg', 'arguments': '{\"search_query\":\"current date\"}'}\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"ttt\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "#ggg=GGG(category=\"ttt\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "response = ggg(\n",
    "    messages=\"user: What is today's date?\\nassistant:\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"gg\",\n",
    "                \"description\": \"The 'gg' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }\n",
    "        }                   \n",
    "    ],\n",
    "    stream=False)\n",
    "print(response.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'content', 'content': '{\\n  \"title\": \"Foundation\",\\n  \"author\": \"Isaac Asimov\",\\n  \"genre\": \"Science Fiction\",\\n  \"series\": {\\n    \"name\": \"Foundation\",\\n    \"type\": \"trilogy\",\\n    \"laterExpandedTo\": \"Foundation series\"\\n  },\\n  \"publication\": {\\n    \"firstBookPublisher\": \"Gnome Press\",\\n    \"firstPublicationYear\": 1951\\n  },\\n  \"description\": \"Foundation is a cycle of five interrelated short stories, first published as a single book in 1951. Collectively, they tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\"\\n}'}\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"ttt\",type=\"gai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "# ggg=GGG(category=\"ttt\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "# Define Grammar\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    summary: str\n",
    "    topic: str\n",
    "    author: str\n",
    "    keywords: list\n",
    "    published_year: int\n",
    "from pydantic import TypeAdapter\n",
    "type_adaptor=TypeAdapter(Book)\n",
    "schema=type_adaptor.json_schema()\n",
    "\n",
    "# Convert unstructured to book record\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "\n",
    "response = ggg(\n",
    "    messages=f\"user: transform this to JSON \\n{text}\\nassistant:\",\n",
    "    schema=schema,\n",
    "    stream=False)\n",
    "print(response.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"ttt\",type=\"gai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "#ggg=GGG(category=\"ttt\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "# Define Grammar\n",
    "schema = {\n",
    "    'type': 'object',\n",
    "    'properties': {\n",
    "        'relevance': {\n",
    "            'type': 'string',\n",
    "            'enum': ['High', 'Medium', 'Low']\n",
    "        },\n",
    "        'reason': {\n",
    "            'type': 'string'\n",
    "        }\n",
    "    },\n",
    "    'required': ['relevance']\n",
    "    }\n",
    "subject=\"Latest News in Singapore\"\n",
    "topic=\"\"\"The New Paper - Breaking News, Sports, Entertainment & Lifestyle News Skip to main content \n",
    "    Toggle navigation The New Paper Search The New Paper Home News Sports Entertainment Lifestyle \n",
    "    Racing Reset Search Top Stories Singapore MFA officer fined for taking nude photos of male \n",
    "    customers Jun 13, 2024 Singapore Man who allegedly removed carpark barrier arms charged Jun 13, 2024 \n",
    "    Singapore Diners brawl with adult shop owner over queueing space Jun 13, 2024 TV Actress Chantalle Ng \n",
    "    treats fans to lunch Jun 13, 2024 Singapore All government agencies to use the same SMS Sender ID \n",
    "    Jun 13, 2024 Team Singapore 5 things about Tan Zong Yang, sprinter Shanti Pereira’s fiance Jun 13, 2024 \n",
    "    Singapore FairPrice shoppers can use vouchers to buy discounted essentials Jun 13, 2024 Singapore \n",
    "    Driver, 74, taken to hospital after car skids, hits bollard Jun 13, 2024 More Stories Movies Young \n",
    "    stars hope I Not Stupid 3 discourages parents from putting pressure on kids Jun 13, 2024 TV 'Er Gu' \n",
    "    goes digital: Veteran\"\"\"\n",
    "response = ggg(\n",
    "    messages=[{'content': f\"\"\"Compare the relevance between {{'Subject': {subject}}} and {{'Topic': {topic} }}. \n",
    "               You should format the result according to this json schema: {schema}\"\"\",\n",
    "               'role': 'user'},\n",
    "              {'content': '', 'role': 'assistant'}],\n",
    "    schema=schema,\n",
    "    stream=False)\n",
    "print(response.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech (TTS)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Build the docker image using the following command in terminal.\n",
    "\n",
    "```bash\n",
    "docker buildx build \\\n",
    "    --build-arg CATEGORY=tts \\\n",
    "    -f ../../gai-sdk/gai-gen/Dockerfile \\\n",
    "    -t gai-tts:latest ../../gai-sdk/gai-gen/\n",
    "```\n",
    "\n",
    "Download model\n",
    "\n",
    "The following demo is uses Coqui AI's xTTS model. Create and run the following script `xtts_download.py` to download the model:\n",
    "\n",
    "```python\n",
    "# xtts_download.py\n",
    "import os\n",
    "os.environ[\"COQUI_TOS_AGREED\"]=\"1\"\n",
    "\n",
    "from TTS.utils.manage import ModelManager\n",
    "print(\"Downloading...\")\n",
    "mm =  ModelManager(output_prefix=\"~/gai/models/tts\")\n",
    "model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
    "mm.download_model(model_name)\n",
    "print(\"Downloaded\")\n",
    "```\n",
    "\n",
    "Take note that loading the model for the first time will take a while for deepspeed to compile the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start service\n",
    "\n",
    "Convert the input text \"The definition of insanity is doing the same thing over and over and expecting different results.\" to speech using the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm -f gai-ttt gai-tts\n",
    "docker run -d \\\n",
    "    -e DEFAULT_GENERATOR=xtts-2 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    -p 12032:12032 \\\n",
    "    --name gai-tts \\\n",
    "    gai-tts:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Coqui xTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"tts\",type=\"gai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "# ggg=GGG(category=\"tts\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "response = ggg(input=\"The definition of insanity is doing the same thing over and over and expecting different results.\",stream=True)\n",
    "\n",
    "from gai.common.sound_utils import play_audio,save_audio\n",
    "play_audio(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Speech-to-Text (STT)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Build the docker image using the following command in terminal.\n",
    "\n",
    "```bash\n",
    "docker buildx build \\\n",
    "    --build-arg CATEGORY=stt \\\n",
    "    -f ../../gai-sdk/gai-gen/Dockerfile \\\n",
    "    -t gai-stt:latest ../../gai-sdk/gai-gen/\n",
    "```\n",
    "\n",
    "Download model\n",
    "\n",
    "```bash\n",
    "mkdir ~/gai/models\n",
    "huggingface-cli download openai/whisper-large-v3 \\\n",
    "        --local-dir ~/gai/models/whisper-large-v3 \\\n",
    "        --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "Start service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm -f gai-ttt gai-tts gai-stt\n",
    "docker run -d \\\n",
    "    -e DEFAULT_GENERATOR=whisper-transformers \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    -p 12033:12033 \\\n",
    "    --name gai-stt \\\n",
    "    gai-stt:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "with open(\"./data/today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    play_audio(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Transcribe audio with local Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"stt\",type=\"gai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "#ggg=GGG(category=\"stt\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "with open(\"./data/today-is-a-wonderful-day.wav\", \"rb\") as f:\n",
    "    output = ggg(file=f)\n",
    "    print(output.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Image-to-Text (ITT)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Build the docker image using the following command in terminal.\n",
    "\n",
    "```bash\n",
    "docker buildx build \\\n",
    "    --build-arg CATEGORY=itt \\\n",
    "    -f ../../gai-sdk/gai-gen/Dockerfile.ITT \\\n",
    "    -t gai-itt:latest ../../gai-sdk/gai-gen/\n",
    "```\n",
    "\n",
    "Download model\n",
    "\n",
    "```bash\n",
    "mkdir ~/gai/models\n",
    "huggingface-cli download liuhaotian/llava-v1.5-7b \\\n",
    "        --local-dir ~/gai/models/llava-v1.5-7b \\\n",
    "        --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "Start service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm -f gai-ttt gai-tts gai-stt gai-itt\n",
    "docker run -d \\\n",
    "    -e DEFAULT_GENERATOR=llava-transformers \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    -p 12034:12034 \\\n",
    "    --name gai-itt \\\n",
    "    gai-itt:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.common.image_utils import read_to_base64\n",
    "import os\n",
    "from IPython.display import Image,display\n",
    "encoded_string = read_to_base64(\"./data/buses.jpeg\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "display(Image(\"./data/buses.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Describe with Llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"itt\",type=\"gai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "# ggg=GGG(category=\"itt\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "# # Llava\n",
    "# from gai.lib.GGG import GGG\n",
    "# ggg=GGG(\"./gai.local.yml\")\n",
    "\n",
    "for chunk in ggg(\n",
    "    messages=messages,\n",
    "    stream=True\n",
    "    ):\n",
    "    print(chunk.decode(),end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text-to-Image (TTI)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Build the docker image using the following command in terminal.\n",
    "\n",
    "```bash\n",
    "docker buildx build \\\n",
    "    --build-arg CATEGORY=tti \\\n",
    "    -f ../../gai-sdk/gai-gen/Dockerfile.TTI \\\n",
    "    -t gai-tti:latest ../../gai-sdk/gai-gen/\n",
    "```\n",
    "\n",
    "Download model\n",
    "\n",
    "```bash\n",
    "mkdir ~/gai/models/Stable-diffusion\n",
    "huggingface-cli download runwayml/stable-diffusion-v1-5 v1-5-pruned-emaonly.safetensors --local-dir ~/gai/models/Stable-diffusion --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm -f gai-ttt gai-tts gai-stt gai-itt gai-tti\n",
    "docker run -d \\\n",
    "    --gpus all \\\n",
    "    -e CLI_ARGS=\"--listen --api --xformers --medvram\" \\\n",
    "    -v ~/gai/models/Stable-diffusion:/stable-diffusion-webui/models/Stable-diffusion \\\n",
    "    -v ~/gai/models/VAE:/stable-diffusion-webui/models/VAE \\\n",
    "    -p 12035:12035 \\\n",
    "    --name gai-tti \\\n",
    "    gai-tti:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Stable Diffusion\n",
    "\n",
    "To check if stable diffusion is running, open browser and browse to \"http://localhost:12035/docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable diffusion\n",
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(category=\"tti\",type=\"gai\",config_path=\"./gai.local.yml\")\n",
    "# Uncomment the line below to use OpenAI's model\n",
    "# ggg=GGG(category=\"tti\",type=\"openai\",config_path=\"./gai.local.yml\")\n",
    "\n",
    "image_data=ggg(\n",
    "    prompt=\"maltese puppy\",\n",
    "    steps=5\n",
    "    )\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "image = Image.open(BytesIO(image_data))\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Build the docker image using the following command in terminal.\n",
    "\n",
    "```bash\n",
    "docker buildx build \\\n",
    "    --build-arg CATEGORY=rag \\\n",
    "    -f ../../gai-sdk/gai-gen/Dockerfile \\\n",
    "    -t gai-rag:latest ../../gai-sdk/gai-gen/\n",
    "```\n",
    "\n",
    "Download model\n",
    "\n",
    "```bash\n",
    "mkdir ~/gai/models\n",
    "huggingface-cli download hkunlp/instructor-large \\\n",
    "        --local-dir ~/gai/models/instructor-large \\\n",
    "        --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm -f gai-ttt gai-tts gai-stt gai-itt gai-tti gai-rag\n",
    "docker run -d \\\n",
    "    -e DEFAULT_GENERATOR=instructor-rag \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    -p 12036:12036 \\\n",
    "    --name gai-rag \\\n",
    "    gai-rag:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run -d \\\n",
    "    -e DEFAULT_GENERATOR=mistral7b-exllama2 \\\n",
    "    --gpus all \\\n",
    "    -v ~/gai/models:/app/models \\\n",
    "    -p 12031:12031 \\\n",
    "    --name gai-ttt \\\n",
    "    gai-ttt:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Index the file 'pm_long_speech_2023.txt' into the vector database. This will break down the file into chunks of 1000 char and convert to text embeddings, then store them in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index 2023 national day speech \n",
    "import asyncio\n",
    "from gai.lib.RAGClientAsync import RAGClientAsync\n",
    "\n",
    "async def listener(status):\n",
    "    print(status)\n",
    "rag=RAGClientAsync(\"./gai.local.yml\")\n",
    "response = await rag.index_document_async(\n",
    "        collection_name=\"demo\",\n",
    "        file_path=\"./data/pm_long_speech_2023.txt\",\n",
    "        title=\"2023 National Day Rally Speech\",\n",
    "        source=\"https://www.pmo.gov.sg/Newsroom/national-day-rally-2023\",\n",
    "        async_callback=listener,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm File Uploaded\n",
    "\n",
    "from gai.lib.RAGClientAsync import RAGClientAsync\n",
    "rag = RAGClientAsync(\"./gai.local.yml\")\n",
    "docs = await rag.list_documents_async()\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Query and Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses 2 models: embedding and text generation.\n",
    "\n",
    "There are 4 scenarios that you can run this:\n",
    "* GPU only: Run both embedding and text generation on GPU. This is the fastest configuration but only if you have enough GPU resources.\n",
    "* GPU + CPU: Run the embedding on GPU and text generation on CPU. This configuration can take up to 2 minutes to complete the retrieval.\n",
    "* GPU + openai: Run the embedding on GPU and openai-gpt4 for text generation. This assumes you have an openai API key.\n",
    "* CPU only: Run both embedding and text generation on CPU. This is the slowest configuration and least preferrable.\n",
    "\n",
    "The following setup is designed for running entirely on GPU, ie. \"gai\". If you have trouble running on GPU, you can try switching \"gai\" to \"openai\" in the command below for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.lib.RAGClientAsync import RAGClientAsync\n",
    "rag = RAGClientAsync(\"./gai.local.yml\")\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"query_texts\": \"Who are the young seniors?\",\n",
    "}\n",
    "response = await rag.retrieve_async(**data)\n",
    "context = response\n",
    "question = \"Who are the young seniors?\"\n",
    "\n",
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"./gai.local.yml\")\n",
    "answer = ggg(\"ttt\", \n",
    "             type=\"gai\", \n",
    "             messages=f\"user: Based on the context below: <context>{context}</context>, answer the question: {question}\\nassistant:\"\n",
    "             )\n",
    "for chunk in answer:\n",
    "    print(chunk.decode(), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query and Retrieve with Function Call\n",
    "\n",
    "The AI will decide based on the context of the conversation, if the response require retrieval to answer the user's query.\n",
    "The following example demonstrates the flow involving function call. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG((\"./gai.local.yml\"))\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag\",\n",
    "            \"description\": \"The \\'rag\\' function is a specialized tool that allows the AI to perform semantic searches on PM Lee Hsien Loong\\'s 2023 National Day Rally. It can be invoked when the AI needs to retrieve facts or information from the speech. This function utilizes advanced Natural Language Processing (NLP) techniques to understand and match the semantic meaning of the user\\'s query with the content of the speech. This is particularly useful when the user\\'s query relates to specific themes, topics, or statements made during the rally.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\":\"string\"\n",
    "                        },\n",
    "                        \"description\": \"An array of query strings to perform a semantic search in the vector database. Each string in the array represents a different way of asking the question. This expands the coverage of the search and increases the chance of finding the best match. For example, instead of using one query like \\'economic policies\\', use multiple variations like [\\'PM Lee Hsien Loong's economic policies announced at the 2023 National Day Rally\\', \\'What were the economic strategies discussed by PM Lee in 2023 National Day Rally?\\', \\'Economic measures announced by PM Lee in 2023 Rally\\'].\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Objective\n",
    "question = \"What did PM Lee said about young seniors?\"\n",
    "\n",
    "# Function Call\n",
    "messages = [{'role':'user','content':question},{'role':'assistant','content':''}]\n",
    "response = ggg(category=\"ttt\",\n",
    "    type=\"openai\",\n",
    "    messages=messages, \n",
    "    tools=tools,\n",
    "    stream=False)\n",
    "result=response.decode()\n",
    "query_texts=result[\"arguments\"]\n",
    "query_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval\n",
    "from gai.lib.RAGClientAsync import RAGClientAsync\n",
    "rag = RAGClientAsync(\"./gai.local.yml\")\n",
    "data = {\n",
    "    \"collection_name\": \"demo\",\n",
    "    \"query_texts\": query_texts,\n",
    "}\n",
    "response = await rag.retrieve_async(**data)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response\n",
    "context = response\n",
    "\n",
    "from gai.lib.GGG import GGG\n",
    "ggg=GGG(\"./gai.local.yml\")\n",
    "answer = ggg(\"ttt\", type=\"openai\", messages=f\"user: Based on the context below: <context>{context}</context>, answer the question: {question}\\nassistant:\")\n",
    "for chunk in answer:\n",
    "    print(chunk.decode(), end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai-lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
