{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c18e4e1",
   "metadata": {},
   "source": [
    "# LM Format Enforcer Integration with TensorRT-LLM\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_trtllm_integration.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook shows how you can integrate with NVIDIA's [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) library, and generate guaranteed JSON-schema-compliant outputs using. The demo focuses on the integration with the library and does not show all capabilities. For a more thorough review of LM Format Enforcer's capabilities, see the [main sample notebook](https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_llama2_enforcer.ipynb).\n",
    "\n",
    "## Setting up the COLAB runtime (user action required)\n",
    "\n",
    "Contrary to other sample notebooks, this notebook requires Colab PRO and will NOT run on the free version. If you manage to find a way to get this demo working on a free Colab node, please reach out :)\n",
    "\n",
    "This colab-pro-friendly notebook is targeted at demoing the enforcer on LLAMA2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabad44",
   "metadata": {},
   "source": [
    "### Installing dependencies\n",
    "\n",
    "This may take a few minutes as tensorrt-llm needs to be installed from source for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update --allow-releaseinfo-change\n",
    "!apt-get update && apt-get -y install openmpi-bin libopenmpi-dev\n",
    "!pip install tensorrt_llm --pre --extra-index-url https://pypi.nvidia.com --extra-index-url https://download.pytorch.org/whl/cu122\n",
    "\n",
    "!pip install pynvml>=11.5.0 lm-format-enforcer huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84eac1",
   "metadata": {},
   "source": [
    "## Gathering huggingface credentials (user action required)\n",
    "\n",
    "This demo uses llama2, so you will have to create a free huggingface account, request access to the llama2 model, create an access token, and insert it when executing the next cell will request it.\n",
    "\n",
    "Links:\n",
    "\n",
    "- [Request access to llama model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). See the \"Access Llama 2 on Hugging Face\" section.\n",
    "- [Create huggingface access token](https://huggingface.co/settings/tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c83eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = snapshot_download(repo_id=\"Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de4691a-f69f-4fa1-b355-8e4cf753a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.9.0.dev2024020600"
     ]
    }
   ],
   "source": [
    "from tensorrt_llm import LLM, ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c831c71d-8469-4806-8e89-ece30038199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: \u001b[1;32m[1/3]\t\u001b[0mLoad HF model to memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a1585911c2498c8ef9e3206221e099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20mTime: 0.881s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[2/3]\t\u001b[0mBuild TRT-LLM engine\n",
      "\u001b[38;20mTime: 62.873s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[3/3]\t\u001b[0mInitialize tokenizer\n",
      "\u001b[38;20mTime: 0.049s\n",
      "\u001b[0m\u001b[1;32mLoading model done.\n",
      "\u001b[0m\u001b[38;20mTotal latency: 63.803s\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(model_dir=model_dir)\n",
    "llm = LLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f8273e-f8e2-491e-9c12-d556a1a7a936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamplingConfig(end_id=2, pad_id=2, max_new_tokens=64, num_beams=1, max_attention_window_size=None, sink_token_length=None, output_sequence_lengths=True, return_dict=True, stop_words_list=None, bad_words_list=None, temperature=1.0, top_k=1, top_p=0.0, top_p_decay=None, top_p_min=None, top_p_reset_ids=None, length_penalty=1.0, repetition_penalty=1.0, min_length=1, presence_penalty=0.0, frequency_penalty=0.0, use_beam_hyps=True, beam_search_diversity_rate=0.0, random_seed=None, output_cum_log_probs=False, output_log_probs=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_config = llm.get_default_sampling_config()\n",
    "sampling_config.max_new_tokens = 64\n",
    "sampling_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f952dc81-70ca-4606-9179-716d2dcaa209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "\n",
    "tokenizer = llm.runtime_context.tokenizer\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant.\n",
    "\"\"\"\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    prompt = f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message}{AnswerFormat.schema_json()} [/INST] '\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66343dfc-9f53-4356-a2b3-7643c870a8c0",
   "metadata": {},
   "source": [
    "# Output without LM Enforcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95dac925-cfb0-406e-8bbc-5b47521746e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorrt_llm/runtime/generation.py:933: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  torch.nested.nested_tensor(split_ids_list,\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Here is the information about Michael Jordan in the requested JSON format:\n",
      "\n",
      "{\n",
      "\"title\": \"AnswerFormat\",\n",
      "\"type\": \"object\",\n",
      "\"properties\": {\n",
      "\"last_name\": {\n",
      "\"title\": \"Last Name\",\n",
      "\"type\": \"string\",\n",
      "\"example\n"
     ]
    }
   ],
   "source": [
    "prompts = [get_prompt('Please give me information about Michael Jordan. You MUST answer using the following json schema: ')]\n",
    "for output in llm.generate(prompts, sampling_config):\n",
    "    print(output.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8441a-f95b-446c-ba90-b2ccff221def",
   "metadata": {},
   "source": [
    "# Output with LM Enforcer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07aa01a-eb07-44cc-a4a6-d66b62fb4659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"last_name\": \"Jordan\",\n",
      "\"year_of_birth\": 1963\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from lmformatenforcer.integrations.trtllm import build_trtllm_logits_processor\n",
    "\n",
    "\n",
    "parser = JsonSchemaParser(AnswerFormat.schema())\n",
    "\n",
    "logits_processor = build_trtllm_logits_processor(tokenizer, parser)\n",
    "\n",
    "inputs = torch.LongTensor(tokenizer.batch_encode_plus(prompts)[\"input_ids\"])\n",
    "\n",
    "out = llm.runtime_context.runtime.generate(inputs, \n",
    "                                     sampling_config = sampling_config,\n",
    "                                     logits_processor=logits_processor)\n",
    "\n",
    "print(tokenizer.decode(logits_processor._trim(out[\"output_ids\"][0][0][len(inputs[0]):])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
