{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# huggingface-cli download turboderp/Llama-3-8B-Instruct-exl2 \\\n",
    "#     --revision 97d15f8fb9808afd51d7bccc3c3204ef3714f65a \\\n",
    "#     --local-dir ~/gai/models/Llama-3-8B-Instruct-exl2 \\\n",
    "#     --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download turboderp/Llama-3-8B-Instruct-exl2 \\\n",
    "    --revision 4.0bpw \\\n",
    "    --local-dir ~/gai/models/Meta-Llama-3-8B-Instruct-EXL2 \\\n",
    "    --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.gen.ttt.ExLlamav2_TTT import ExLlamav2_TTT\n",
    "config = {\n",
    "            \"model_name\": \"ttt-exllama2-llama3\",\n",
    "            \"type\": \"ttt\",\n",
    "            \"engine\": \"ExLlamaV2_TTT\",\n",
    "            \"model_path\": \"models/Meta-Llama-3-8B-Instruct-EXL2\",\n",
    "            \"model_basename\": \"model\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop_conditions\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"temperature\": 0.85,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 50,\n",
    "                \"max_new_tokens\": 100,\n",
    "            },\n",
    "            \"no_flash_attn\":True,\n",
    "            \"seed\": None\n",
    "        }    \n",
    "gen = ExLlamav2_TTT(config)\n",
    "gen.load()\n",
    "response = gen.create(messages=[\n",
    "    {'role':'system','content':'You are a helpful assistant that can generate short stories. You can generate a short story based on a prompt.'},\n",
    "    {'role':'user','content':'Tell me a one paragraph short story.'},\n",
    "    {'role':'assistant','content':''}],\n",
    "    stream=False)\n",
    "gen.unload()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.gen.ttt.ExLlamav2_TTT import ExLlamav2_TTT\n",
    "config = {\n",
    "            \"model_name\": \"ttt-exllama2-llama3\",\n",
    "            \"type\": \"ttt\",\n",
    "            \"engine\": \"ExLlamaV2_TTT\",\n",
    "            \"model_path\": \"models/Meta-Llama-3-8B-Instruct-EXL2\",\n",
    "            \"model_basename\": \"model\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop_conditions\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"temperature\": 0.85,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 50,\n",
    "                \"max_new_tokens\": 500,\n",
    "            },\n",
    "            \"no_flash_attn\":True,\n",
    "            \"seed\": None\n",
    "        } \n",
    "gen = ExLlamav2_TTT(config)\n",
    "gen.load()\n",
    "response = gen.create(messages=[\n",
    "    {'role':'system','content':'You are a helpful assistant that can generate short stories. You can generate a short story based on a prompt.'},\n",
    "    {'role':'user','content':'Tell me a one paragraph short story.'},\n",
    "    {'role':'assistant','content':''}],\n",
    "    stream=True)\n",
    "for chunk in response:\n",
    "    chunk=chunk.choices[0].delta.content\n",
    "    if chunk:\n",
    "        print(chunk, end='', flush=True)\n",
    "gen.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gai.gen.ttt.ExLlamav2_TTT import ExLlamav2_TTT\n",
    "config = {\n",
    "            \"model_name\": \"ttt-exllama2-llama3\",\n",
    "            \"type\": \"ttt\",\n",
    "            \"engine\": \"ExLlamaV2_TTT\",\n",
    "            \"model_path\": \"models/Meta-Llama-3-8B-Instruct-EXL2\",\n",
    "            \"model_basename\": \"model\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop_conditions\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"temperature\": 0.85,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 50,\n",
    "                \"max_new_tokens\": 400,\n",
    "            },\n",
    "            \"no_flash_attn\":True,\n",
    "            \"seed\": None\n",
    "        } \n",
    "gen = ExLlamav2_TTT(config)\n",
    "gen.load()\n",
    "\n",
    "# Define Schema\n",
    "from pydantic import BaseModel\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "response = gen.create(messages=[{'role':'user','content':text},{'role':'assistant','content':''}], \n",
    "    schema=Book.schema()\n",
    "    )\n",
    "gen.unload()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 18:05:22 INFO gai.gen.ttt.ExLlamav2_TTT:\u001b[32mExLlama_TTT2.load: Loading model from /home/roylai/gai/models/Meta-Llama-3-8B-Instruct-EXL2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 18:05:34 INFO gai.gen.ttt.ExLlamav2_TTT:\u001b[32mExLlama_TTT2.create:\n",
      "\tprompt=`<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\n",
      "            1. Review the <tools> below and assess if any of them is suitable for responding to the user's message.\n",
      "\n",
      "                [{'type': 'function', 'function': {'name': 'google', 'description': \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}]\n",
      "\n",
      "            2. If none of the tools are suitable, you can respond with a <text> response that looks like the following:\n",
      "                \n",
      "            {\n",
      "                \"function\": {\n",
      "                    \"name\": \"text\",\n",
      "                    \"arguments\": {\n",
      "                        \"text\": \"This is a text response.\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "            <|eot_id|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Begin your response with an open curly brace. Your response must be parseable by this json schema:\n",
      "        ```jsonschema\n",
      "        {'type': 'object', 'properties': {'function': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'object'}}, 'required': ['name', 'arguments']}}, 'required': ['function']}\n",
      "        ```<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What time is it in Singapore right now?<|eot_id|><|start_header_id|>assistant<|end_header_id|>`\n",
      "\tschema=`{'type': 'object', 'properties': {'function': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'object'}}, 'required': ['name', 'arguments']}}, 'required': ['function']}`\n",
      "\ttools=`[{'type': 'function', 'function': {'name': 'google', 'description': \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}]`\n",
      "\tprompt_format=`llama3`\u001b[0m\n",
      "2024-06-30 18:05:38 DEBUG gai.gen.ttt.ExLlamav2_TTT:\u001b[35mExLlama_TTT2._generating: function_name=google function_arguments={\"search_query\": \"current time in Singapore\"}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-6e97190b-86e5-49ba-9df0-07f1a648f7fe', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_eedd6b78-c865-4d1b-acce-644484787909', function=Function(arguments='{\"search_query\": \"current time in Singapore\"}', name='google'), type='function')]))], created=1719741938, model='ttt-exllama2-llama3', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=29, prompt_tokens=444, total_tokens=473))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided not to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 18:05:38 INFO gai.gen.ttt.ExLlamav2_TTT:\u001b[32mExLlama_TTT2.create:\n",
      "\tprompt=`<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\n",
      "            1. Review the <tools> below and assess if any of them is suitable for responding to the user's message.\n",
      "\n",
      "                [{'type': 'function', 'function': {'name': 'google', 'description': \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}]\n",
      "\n",
      "            2. If none of the tools are suitable, you can respond with a <text> response that looks like the following:\n",
      "                \n",
      "            {\n",
      "                \"function\": {\n",
      "                    \"name\": \"text\",\n",
      "                    \"arguments\": {\n",
      "                        \"text\": \"This is a text response.\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "            <|eot_id|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Begin your response with an open curly brace. Your response must be parseable by this json schema:\n",
      "        ```jsonschema\n",
      "        {'type': 'object', 'properties': {'function': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'object'}}, 'required': ['name', 'arguments']}}, 'required': ['function']}\n",
      "        ```<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a one paragraph story.<|eot_id|><|start_header_id|>assistant<|end_header_id|>`\n",
      "\tschema=`{'type': 'object', 'properties': {'function': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'object'}}, 'required': ['name', 'arguments']}}, 'required': ['function']}`\n",
      "\ttools=`[{'type': 'function', 'function': {'name': 'google', 'description': \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\", 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"}}, 'required': ['search_query']}}}]`\n",
      "\tprompt_format=`llama3`\u001b[0m\n",
      "2024-06-30 18:05:43 WARNING gai.gen.ttt.ExLlamav2_TTT:\u001b[33mExLlamav2_TTT._generating: Error loading JSON for schema. Expecting ',' delimiter: line 3 column 706 (char 707)\u001b[0m\n",
      "2024-06-30 18:05:47 WARNING gai.gen.ttt.ExLlamav2_TTT:\u001b[33mExLlamav2_TTT._generating: Error loading JSON for schema. Expecting ',' delimiter: line 3 column 706 (char 707)\u001b[0m\n",
      "2024-06-30 18:05:51 WARNING gai.gen.ttt.ExLlamav2_TTT:\u001b[33mExLlamav2_TTT._generating: Error loading JSON for schema. Expecting ',' delimiter: line 3 column 706 (char 707)\u001b[0m\n",
      "2024-06-30 18:05:55 WARNING gai.gen.ttt.ExLlamav2_TTT:\u001b[33mExLlamav2_TTT._generating: Error loading JSON for schema. Expecting ',' delimiter: line 3 column 706 (char 707)\u001b[0m\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to generate JSON for schema after maximum number of retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m highlight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel decided not to use tool: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a one paragraph story.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     65\u001b[0m gen\u001b[38;5;241m.\u001b[39munload()\n",
      "File \u001b[0;32m~/github/gai-labs/gai/gai-sdk/gai-gen/gai/gen/ttt/ExLlamav2_TTT.py:434\u001b[0m, in \u001b[0;36mExLlamav2_TTT.create\u001b[0;34m(self, messages, stream, max_new_tokens, temperature, top_k, top_p, tools, tool_choice, schema)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tools \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m schema:\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (chunk \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming(\n\u001b[1;32m    428\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    429\u001b[0m         settings\u001b[38;5;241m=\u001b[39msettings,\n\u001b[1;32m    430\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m    431\u001b[0m         stop_conditions\u001b[38;5;241m=\u001b[39mstop_conditions,\n\u001b[1;32m    432\u001b[0m     ))\n\u001b[0;32m--> 434\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generating\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_conditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_conditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/github/gai-labs/gai/gai-sdk/gai-gen/gai/gen/ttt/ExLlamav2_TTT.py:319\u001b[0m, in \u001b[0;36mExLlamav2_TTT._generating\u001b[0;34m(self, prompt, settings, max_new_tokens, schema, tools, stop_conditions, seed)\u001b[0m\n\u001b[1;32m    316\u001b[0m             retries\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to generate JSON for schema after maximum number of retries.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    321\u001b[0m finish_reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m        \n\u001b[1;32m    322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(response)\n",
      "\u001b[0;31mException\u001b[0m: Failed to generate JSON for schema after maximum number of retries."
     ]
    }
   ],
   "source": [
    "from gai.gen.ttt.ExLlamav2_TTT import ExLlamav2_TTT\n",
    "config = {\n",
    "            \"model_name\": \"ttt-exllama2-llama3\",\n",
    "            \"type\": \"ttt\",\n",
    "            \"engine\": \"ExLlamaV2_TTT\",\n",
    "            \"model_path\": \"models/Meta-Llama-3-8B-Instruct-EXL2\",\n",
    "            \"model_basename\": \"model\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop_conditions\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.1,\n",
    "                \"top_k\": 50,\n",
    "                \"max_new_tokens\": 1000,\n",
    "            },\n",
    "            \"no_flash_attn\":True,\n",
    "            \"seed\": None\n",
    "        } \n",
    "gen = ExLlamav2_TTT(config)\n",
    "gen.load()\n",
    "\n",
    "from gai.common.notebook import highlight\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google\",\n",
    "            \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "highlight(\"Model decided to use tool: \")\n",
    "user_prompt = \"What time is it in Singapore right now?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "    tools=tools,\n",
    "    stream=False,\n",
    "    max_new_tokens=1000)\n",
    "print(response)\n",
    "\n",
    "highlight(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "    tools=tools,\n",
    "    stream=False,\n",
    "    max_new_tokens=1000)\n",
    "print(response)\n",
    "\n",
    "gen.unload()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
