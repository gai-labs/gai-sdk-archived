{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "huggingface-cli download mys/ggml_llava-v1.5-7b \\\n",
    "                mmproj-model-f16.gguf  \\\n",
    "                ggml-model-q4_k.gguf  \\\n",
    "                --local-dir ~/gai/models/ggml_llava-v1.5-7b \\\n",
    "                --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 21:49:24 INFO gai.gen.ttt.LlamaCpp_TTT:\u001b[32mexllama_engine.load: Loading model from /home/roylai/gai/models/ggml_llava-v1.5-7b/ggml-model-q4_k.gguf\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-77fc73c6-c88b-4242-8825-6d3e34272140', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='\\n\\n[0.12, 0.74, 0.35, 0.86]\\n[0.12, 0.74, 0.35, 0.86]\\n[0.12, 0.74, 0.35, 0.86]\\n[0.12, 0.74, 0.35, 0.86', role='assistant', function_call=None, tool_calls=None))], created=1718200194, model='llamacpp-llava', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=102, prompt_tokens=20194, total_tokens=20296))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gai.gen.ttt.LlamaCpp_TTT import LlamaCpp_TTT\n",
    "config = {\n",
    "            \"type\": \"itt\",\n",
    "            \"model_name\": \"llamacpp-llava\",\n",
    "            \"engine\": \"LlamaCpp_ITT\",\n",
    "            \"model_filepath\": \"models/ggml_llava-v1.5-7b/ggml-model-q4_k.gguf\",\n",
    "            \"clip_model_path\": \"models/ggml_llava-v1.5-7b/mmproj-model-f16.gguf\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\": 1.31,\n",
    "                \"top_k\": 49,\n",
    "                \"top_p\": 0.14\n",
    "            }\n",
    "        }   \n",
    "gen = LlamaCpp_TTT(config)\n",
    "gen.load()\n",
    "import base64\n",
    "encoded_string = \"\"\n",
    "with open(\"./buses.jpeg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "response = gen.create(messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Whatâ€™s in this image?\"},\n",
    "                    {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encoded_string}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4000,\n",
    "    stream=False)\n",
    "gen.unload()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 20:05:03 INFO gai.gen.ttt.LlamaCpp_TTT:\u001b[32mexllama_engine.load: Loading model from /home/roylai/gai/models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the quaint village of Willowbrook, nestled between rolling hills and a sparkling river, there lived an old woman named Elsie. She was known for her kind heart and wise words, often sharing stories with the children at the local school. One sunny afternoon, as she tended to her garden, Elsie discovered a tiny, lost bird perched on a flower petal. With gentle care, she coaxed it into her hand, naming him Whispers. Over time, their bond grew strong; Whispers would visit Elsie daily, bringing joy and comfort to the villagers who marveled at this extraordinary friendship between an old woman and a small bird. And so, in Willowbrook, love transcended boundaries, proving that even the smallest creatures can touch our hearts and remind us of life's simple beauty."
     ]
    }
   ],
   "source": [
    "from gai.gen.ttt.LlamaCpp_TTT import LlamaCpp_TTT\n",
    "config = {\n",
    "            \"type\": \"ttt\",\n",
    "            \"model_name\": \"Llama3-LlamaCpp\",\n",
    "            \"engine\": \"LlamaCpp_TTT\",\n",
    "            \"model_filepath\": \"models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\": 1.31,\n",
    "                \"top_k\": 49,\n",
    "                \"top_p\": 0.14\n",
    "            }\n",
    "        }   \n",
    "gen = LlamaCpp_TTT(config)\n",
    "gen.load()\n",
    "response = gen.create(messages=[\n",
    "    {'role':'system','content':'You are a helpful assistant that can generate short stories. You can generate a short story based on a prompt.'},\n",
    "    {'role':'user','content':'Tell me a one paragraph short story.'},\n",
    "    {'role':'assistant','content':''}],\n",
    "    max_tokens=500,\n",
    "    stream=True,\n",
    "    seed=1234)\n",
    "for chunk in response:\n",
    "    chunk=chunk.choices[0].delta.content\n",
    "    if chunk:\n",
    "        print(chunk, end='', flush=True)\n",
    "gen.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 20:05:55 INFO gai.gen.ttt.LlamaCpp_TTT:\u001b[32mexllama_engine.load: Loading model from /home/roylai/gai/models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\u001b[0m\n",
      "from_string grammar:\n",
      "author-kv ::= [\"] [a] [u] [t] [h] [o] [r] [\"] space [:] space string \n",
      "space ::= space_43 \n",
      "string ::= [\"] string_44 [\"] space \n",
      "char ::= [^\"\\] | [\\] char_4 \n",
      "char_4 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "integer ::= integer_6 space \n",
      "integer_6 ::= integer_7 integral-part \n",
      "integer_7 ::= [-] | \n",
      "integral-part ::= [0-9] | [1-9] integral-part_38 \n",
      "integral-part_9 ::= [0-9] integral-part_37 \n",
      "integral-part_10 ::= [0-9] integral-part_36 \n",
      "integral-part_11 ::= [0-9] integral-part_35 \n",
      "integral-part_12 ::= [0-9] integral-part_34 \n",
      "integral-part_13 ::= [0-9] integral-part_33 \n",
      "integral-part_14 ::= [0-9] integral-part_32 \n",
      "integral-part_15 ::= [0-9] integral-part_31 \n",
      "integral-part_16 ::= [0-9] integral-part_30 \n",
      "integral-part_17 ::= [0-9] integral-part_29 \n",
      "integral-part_18 ::= [0-9] integral-part_28 \n",
      "integral-part_19 ::= [0-9] integral-part_27 \n",
      "integral-part_20 ::= [0-9] integral-part_26 \n",
      "integral-part_21 ::= [0-9] integral-part_25 \n",
      "integral-part_22 ::= [0-9] integral-part_24 \n",
      "integral-part_23 ::= [0-9] \n",
      "integral-part_24 ::= integral-part_23 | \n",
      "integral-part_25 ::= integral-part_22 | \n",
      "integral-part_26 ::= integral-part_21 | \n",
      "integral-part_27 ::= integral-part_20 | \n",
      "integral-part_28 ::= integral-part_19 | \n",
      "integral-part_29 ::= integral-part_18 | \n",
      "integral-part_30 ::= integral-part_17 | \n",
      "integral-part_31 ::= integral-part_16 | \n",
      "integral-part_32 ::= integral-part_15 | \n",
      "integral-part_33 ::= integral-part_14 | \n",
      "integral-part_34 ::= integral-part_13 | \n",
      "integral-part_35 ::= integral-part_12 | \n",
      "integral-part_36 ::= integral-part_11 | \n",
      "integral-part_37 ::= integral-part_10 | \n",
      "integral-part_38 ::= integral-part_9 | \n",
      "published-year-kv ::= [\"] [p] [u] [b] [l] [i] [s] [h] [e] [d] [_] [y] [e] [a] [r] [\"] space [:] space integer \n",
      "root ::= [{] space title-kv [,] space summary-kv [,] space author-kv [,] space published-year-kv [}] space \n",
      "title-kv ::= [\"] [t] [i] [t] [l] [e] [\"] space [:] space string \n",
      "summary-kv ::= [\"] [s] [u] [m] [m] [a] [r] [y] [\"] space [:] space string \n",
      "space_43 ::= [ ] | \n",
      "string_44 ::= char string_44 | \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-40d25c24-51f7-4d80-a1c6-289c1457da89', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"title\": \"Foundation\", \"summary\": \"Foundation is a seminal science fiction novel written by American author Isaac Asimov. It was first published in 1951 as part of his Foundation Trilogy, which later expanded into the larger Foundation series. The book comprises five interconnected short stories that collectively narrate the early history of the Foundation, an institution established by psychohistorian Hari Seldon to safeguard and preserve the most valuable aspects of galactic civilization after the anticipated fall of the Galactic Empire.\\n\\nThe', role='assistant', function_call=None, tool_calls=None))], created=1718193994, model='Llama3-LlamaCpp', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=101, prompt_tokens=102, total_tokens=203))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gai.gen.ttt.LlamaCpp_TTT import LlamaCpp_TTT\n",
    "config = {\n",
    "            \"type\": \"ttt\",\n",
    "            \"model_name\": \"Llama3-LlamaCpp\",\n",
    "            \"engine\": \"LlamaCpp_TTT\",\n",
    "            \"model_filepath\": \"models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\": 1.31,\n",
    "                \"top_k\": 49,\n",
    "                \"top_p\": 0.14\n",
    "            }\n",
    "        }   \n",
    "gen = LlamaCpp_TTT(config)\n",
    "gen.load()\n",
    "\n",
    "# Define Schema\n",
    "from pydantic import BaseModel\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "response = gen.create(messages=[{'role':'user','content':text},{'role':'assistant','content':''}], \n",
    "    schema=Book.schema()\n",
    "    )\n",
    "gen.unload()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 20:09:04 INFO gai.gen.ttt.LlamaCpp_TTT:\u001b[32mexllama_engine.load: Loading model from /home/roylai/gai/models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "array ::= [[] space array_6 []] space \n",
      "space ::= space_94 \n",
      "array_2 ::= value array_5 \n",
      "value ::= object | array | string | number | boolean | null \n",
      "array_4 ::= [,] space value \n",
      "array_5 ::= array_4 array_5 | \n",
      "array_6 ::= array_2 | \n",
      "boolean ::= boolean_8 space \n",
      "boolean_8 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "char ::= [^\"\\] | [\\] char_10 \n",
      "char_10 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "decimal-part ::= [0-9] decimal-part_41 \n",
      "decimal-part_12 ::= [0-9] decimal-part_40 \n",
      "decimal-part_13 ::= [0-9] decimal-part_39 \n",
      "decimal-part_14 ::= [0-9] decimal-part_38 \n",
      "decimal-part_15 ::= [0-9] decimal-part_37 \n",
      "decimal-part_16 ::= [0-9] decimal-part_36 \n",
      "decimal-part_17 ::= [0-9] decimal-part_35 \n",
      "decimal-part_18 ::= [0-9] decimal-part_34 \n",
      "decimal-part_19 ::= [0-9] decimal-part_33 \n",
      "decimal-part_20 ::= [0-9] decimal-part_32 \n",
      "decimal-part_21 ::= [0-9] decimal-part_31 \n",
      "decimal-part_22 ::= [0-9] decimal-part_30 \n",
      "decimal-part_23 ::= [0-9] decimal-part_29 \n",
      "decimal-part_24 ::= [0-9] decimal-part_28 \n",
      "decimal-part_25 ::= [0-9] decimal-part_27 \n",
      "decimal-part_26 ::= [0-9] \n",
      "decimal-part_27 ::= decimal-part_26 | \n",
      "decimal-part_28 ::= decimal-part_25 | \n",
      "decimal-part_29 ::= decimal-part_24 | \n",
      "decimal-part_30 ::= decimal-part_23 | \n",
      "decimal-part_31 ::= decimal-part_22 | \n",
      "decimal-part_32 ::= decimal-part_21 | \n",
      "decimal-part_33 ::= decimal-part_20 | \n",
      "decimal-part_34 ::= decimal-part_19 | \n",
      "decimal-part_35 ::= decimal-part_18 | \n",
      "decimal-part_36 ::= decimal-part_17 | \n",
      "decimal-part_37 ::= decimal-part_16 | \n",
      "decimal-part_38 ::= decimal-part_15 | \n",
      "decimal-part_39 ::= decimal-part_14 | \n",
      "decimal-part_40 ::= decimal-part_13 | \n",
      "decimal-part_41 ::= decimal-part_12 | \n",
      "function ::= [{] space function-name-kv [,] space function-arguments-kv [}] space \n",
      "function-name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
      "function-arguments-kv ::= [\"] [a] [r] [g] [u] [m] [e] [n] [t] [s] [\"] space [:] space function-arguments \n",
      "function-arguments ::= object \n",
      "object ::= [{] space object_92 [}] space \n",
      "function-kv ::= [\"] [f] [u] [n] [c] [t] [i] [o] [n] [\"] space [:] space function \n",
      "string ::= [\"] string_95 [\"] space \n",
      "integral-part ::= [0-9] | [1-9] integral-part_79 \n",
      "integral-part_50 ::= [0-9] integral-part_78 \n",
      "integral-part_51 ::= [0-9] integral-part_77 \n",
      "integral-part_52 ::= [0-9] integral-part_76 \n",
      "integral-part_53 ::= [0-9] integral-part_75 \n",
      "integral-part_54 ::= [0-9] integral-part_74 \n",
      "integral-part_55 ::= [0-9] integral-part_73 \n",
      "integral-part_56 ::= [0-9] integral-part_72 \n",
      "integral-part_57 ::= [0-9] integral-part_71 \n",
      "integral-part_58 ::= [0-9] integral-part_70 \n",
      "integral-part_59 ::= [0-9] integral-part_69 \n",
      "integral-part_60 ::= [0-9] integral-part_68 \n",
      "integral-part_61 ::= [0-9] integral-part_67 \n",
      "integral-part_62 ::= [0-9] integral-part_66 \n",
      "integral-part_63 ::= [0-9] integral-part_65 \n",
      "integral-part_64 ::= [0-9] \n",
      "integral-part_65 ::= integral-part_64 | \n",
      "integral-part_66 ::= integral-part_63 | \n",
      "integral-part_67 ::= integral-part_62 | \n",
      "integral-part_68 ::= integral-part_61 | \n",
      "integral-part_69 ::= integral-part_60 | \n",
      "integral-part_70 ::= integral-part_59 | \n",
      "integral-part_71 ::= integral-part_58 | \n",
      "integral-part_72 ::= integral-part_57 | \n",
      "integral-part_73 ::= integral-part_56 | \n",
      "integral-part_74 ::= integral-part_55 | \n",
      "integral-part_75 ::= integral-part_54 | \n",
      "integral-part_76 ::= integral-part_53 | \n",
      "integral-part_77 ::= integral-part_52 | \n",
      "integral-part_78 ::= integral-part_51 | \n",
      "integral-part_79 ::= integral-part_50 | \n",
      "null ::= [n] [u] [l] [l] space \n",
      "number ::= number_82 number_85 number_88 space \n",
      "number_82 ::= number_83 integral-part \n",
      "number_83 ::= [-] | \n",
      "number_84 ::= [.] decimal-part \n",
      "number_85 ::= number_84 | \n",
      "number_86 ::= [eE] number_87 integral-part \n",
      "number_87 ::= [-+] | \n",
      "number_88 ::= number_86 | \n",
      "object_89 ::= string [:] space value object_91 \n",
      "object_90 ::= [,] space string [:] space value \n",
      "object_91 ::= object_90 object_91 | \n",
      "object_92 ::= object_89 | \n",
      "root ::= [{] space function-kv [}] space \n",
      "space_94 ::= [ ] | \n",
      "string_95 ::= char string_95 | \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-f3eabcbe-a353-42cd-817f-7f5e1bc7c187', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{ \"function\": { \"name\": \"google\", \"arguments\": { \"search_query\": \"current time in Singapore\" } } }', role='assistant', function_call=None, tool_calls=None))], created=1718194176, model='Llama3-LlamaCpp', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=29, prompt_tokens=345, total_tokens=374))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Model decided not to use tool: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "array ::= [[] space array_6 []] space \n",
      "space ::= space_94 \n",
      "array_2 ::= value array_5 \n",
      "value ::= object | array | string | number | boolean | null \n",
      "array_4 ::= [,] space value \n",
      "array_5 ::= array_4 array_5 | \n",
      "array_6 ::= array_2 | \n",
      "boolean ::= boolean_8 space \n",
      "boolean_8 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "char ::= [^\"\\] | [\\] char_10 \n",
      "char_10 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "decimal-part ::= [0-9] decimal-part_41 \n",
      "decimal-part_12 ::= [0-9] decimal-part_40 \n",
      "decimal-part_13 ::= [0-9] decimal-part_39 \n",
      "decimal-part_14 ::= [0-9] decimal-part_38 \n",
      "decimal-part_15 ::= [0-9] decimal-part_37 \n",
      "decimal-part_16 ::= [0-9] decimal-part_36 \n",
      "decimal-part_17 ::= [0-9] decimal-part_35 \n",
      "decimal-part_18 ::= [0-9] decimal-part_34 \n",
      "decimal-part_19 ::= [0-9] decimal-part_33 \n",
      "decimal-part_20 ::= [0-9] decimal-part_32 \n",
      "decimal-part_21 ::= [0-9] decimal-part_31 \n",
      "decimal-part_22 ::= [0-9] decimal-part_30 \n",
      "decimal-part_23 ::= [0-9] decimal-part_29 \n",
      "decimal-part_24 ::= [0-9] decimal-part_28 \n",
      "decimal-part_25 ::= [0-9] decimal-part_27 \n",
      "decimal-part_26 ::= [0-9] \n",
      "decimal-part_27 ::= decimal-part_26 | \n",
      "decimal-part_28 ::= decimal-part_25 | \n",
      "decimal-part_29 ::= decimal-part_24 | \n",
      "decimal-part_30 ::= decimal-part_23 | \n",
      "decimal-part_31 ::= decimal-part_22 | \n",
      "decimal-part_32 ::= decimal-part_21 | \n",
      "decimal-part_33 ::= decimal-part_20 | \n",
      "decimal-part_34 ::= decimal-part_19 | \n",
      "decimal-part_35 ::= decimal-part_18 | \n",
      "decimal-part_36 ::= decimal-part_17 | \n",
      "decimal-part_37 ::= decimal-part_16 | \n",
      "decimal-part_38 ::= decimal-part_15 | \n",
      "decimal-part_39 ::= decimal-part_14 | \n",
      "decimal-part_40 ::= decimal-part_13 | \n",
      "decimal-part_41 ::= decimal-part_12 | \n",
      "function ::= [{] space function-name-kv [,] space function-arguments-kv [}] space \n",
      "function-name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
      "function-arguments-kv ::= [\"] [a] [r] [g] [u] [m] [e] [n] [t] [s] [\"] space [:] space function-arguments \n",
      "function-arguments ::= object \n",
      "object ::= [{] space object_92 [}] space \n",
      "function-kv ::= [\"] [f] [u] [n] [c] [t] [i] [o] [n] [\"] space [:] space function \n",
      "string ::= [\"] string_95 [\"] space \n",
      "integral-part ::= [0-9] | [1-9] integral-part_79 \n",
      "integral-part_50 ::= [0-9] integral-part_78 \n",
      "integral-part_51 ::= [0-9] integral-part_77 \n",
      "integral-part_52 ::= [0-9] integral-part_76 \n",
      "integral-part_53 ::= [0-9] integral-part_75 \n",
      "integral-part_54 ::= [0-9] integral-part_74 \n",
      "integral-part_55 ::= [0-9] integral-part_73 \n",
      "integral-part_56 ::= [0-9] integral-part_72 \n",
      "integral-part_57 ::= [0-9] integral-part_71 \n",
      "integral-part_58 ::= [0-9] integral-part_70 \n",
      "integral-part_59 ::= [0-9] integral-part_69 \n",
      "integral-part_60 ::= [0-9] integral-part_68 \n",
      "integral-part_61 ::= [0-9] integral-part_67 \n",
      "integral-part_62 ::= [0-9] integral-part_66 \n",
      "integral-part_63 ::= [0-9] integral-part_65 \n",
      "integral-part_64 ::= [0-9] \n",
      "integral-part_65 ::= integral-part_64 | \n",
      "integral-part_66 ::= integral-part_63 | \n",
      "integral-part_67 ::= integral-part_62 | \n",
      "integral-part_68 ::= integral-part_61 | \n",
      "integral-part_69 ::= integral-part_60 | \n",
      "integral-part_70 ::= integral-part_59 | \n",
      "integral-part_71 ::= integral-part_58 | \n",
      "integral-part_72 ::= integral-part_57 | \n",
      "integral-part_73 ::= integral-part_56 | \n",
      "integral-part_74 ::= integral-part_55 | \n",
      "integral-part_75 ::= integral-part_54 | \n",
      "integral-part_76 ::= integral-part_53 | \n",
      "integral-part_77 ::= integral-part_52 | \n",
      "integral-part_78 ::= integral-part_51 | \n",
      "integral-part_79 ::= integral-part_50 | \n",
      "null ::= [n] [u] [l] [l] space \n",
      "number ::= number_82 number_85 number_88 space \n",
      "number_82 ::= number_83 integral-part \n",
      "number_83 ::= [-] | \n",
      "number_84 ::= [.] decimal-part \n",
      "number_85 ::= number_84 | \n",
      "number_86 ::= [eE] number_87 integral-part \n",
      "number_87 ::= [-+] | \n",
      "number_88 ::= number_86 | \n",
      "object_89 ::= string [:] space value object_91 \n",
      "object_90 ::= [,] space string [:] space value \n",
      "object_91 ::= object_90 object_91 | \n",
      "object_92 ::= object_89 | \n",
      "root ::= [{] space function-kv [}] space \n",
      "space_94 ::= [ ] | \n",
      "string_95 ::= char string_95 | \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8e4a4cb0-d9ef-41ad-a2eb-0ee0fa121f6f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{ \"function\": { \"name\": \"google\", \"arguments\": { \"search_query\": \"one paragraph story\" } } }', role='assistant', function_call=None, tool_calls=None))], created=1718194185, model='Llama3-LlamaCpp', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=343, total_tokens=371))\n"
     ]
    }
   ],
   "source": [
    "from gai.gen.ttt.LlamaCpp_TTT import LlamaCpp_TTT\n",
    "config = {\n",
    "            \"type\": \"ttt\",\n",
    "            \"model_name\": \"Llama3-LlamaCpp\",\n",
    "            \"engine\": \"LlamaCpp_TTT\",\n",
    "            \"model_filepath\": \"models/LLaMA3-iterative-DPO-final-GGUF/LLaMA3-iterative-DPO-final-Q4_K_M.gguf\",\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"prompt_format\": \"llama3\",\n",
    "            \"stop\": [\"<|eot_id|>\"],\n",
    "            \"hyperparameters\": {\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\": 1.31,\n",
    "                \"top_k\": 49,\n",
    "                \"top_p\": 0.14\n",
    "            }\n",
    "        }   \n",
    "gen = LlamaCpp_TTT(config)\n",
    "gen.load()\n",
    "\n",
    "from gai.common.notebook import highlight\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google\",\n",
    "            \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "highlight(\"Model decided to use tool: \")\n",
    "user_prompt = \"What time is it in Singapore right now?\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "    tools=tools,\n",
    "    stream=False,\n",
    "    max_tokens=200)\n",
    "print(response)\n",
    "\n",
    "highlight(\"Model decided not to use tool: \")\n",
    "user_prompt = \"Tell me a one paragraph story.\"\n",
    "response = gen.create(\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "        {'role':'assistant','content':''}],\n",
    "    tools=tools,\n",
    "    stream=False,\n",
    "    max_tokens=200)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
